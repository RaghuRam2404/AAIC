<!DOCTYPE html>
<html>

<head>

<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<title>1 Real world problem- Predict rating given product reviews on Amazon</title>


<style type="text/css">
body {
  font-family: Helvetica, arial, sans-serif;
  font-size: 14px;
  line-height: 1.6;
  padding-top: 10px;
  padding-bottom: 10px;
  background-color: white;
  padding: 30px; }

body > *:first-child {
  margin-top: 0 !important; }
body > *:last-child {
  margin-bottom: 0 !important; }

a {
  color: #4183C4; }
a.absent {
  color: #cc0000; }
a.anchor {
  display: block;
  padding-left: 30px;
  margin-left: -30px;
  cursor: pointer;
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0; }

h1, h2, h3, h4, h5, h6 {
  margin: 20px 0 10px;
  padding: 0;
  font-weight: bold;
  -webkit-font-smoothing: antialiased;
  cursor: text;
  position: relative; }

h1:hover a.anchor, h2:hover a.anchor, h3:hover a.anchor, h4:hover a.anchor, h5:hover a.anchor, h6:hover a.anchor {
  background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA09pVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMy1jMDExIDY2LjE0NTY2MSwgMjAxMi8wMi8wNi0xNDo1NjoyNyAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvIiB4bWxuczp4bXBNTT0iaHR0cDovL25zLmFkb2JlLmNvbS94YXAvMS4wL21tLyIgeG1sbnM6c3RSZWY9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9zVHlwZS9SZXNvdXJjZVJlZiMiIHhtcDpDcmVhdG9yVG9vbD0iQWRvYmUgUGhvdG9zaG9wIENTNiAoMTMuMCAyMDEyMDMwNS5tLjQxNSAyMDEyLzAzLzA1OjIxOjAwOjAwKSAgKE1hY2ludG9zaCkiIHhtcE1NOkluc3RhbmNlSUQ9InhtcC5paWQ6OUM2NjlDQjI4ODBGMTFFMTg1ODlEODNERDJBRjUwQTQiIHhtcE1NOkRvY3VtZW50SUQ9InhtcC5kaWQ6OUM2NjlDQjM4ODBGMTFFMTg1ODlEODNERDJBRjUwQTQiPiA8eG1wTU06RGVyaXZlZEZyb20gc3RSZWY6aW5zdGFuY2VJRD0ieG1wLmlpZDo5QzY2OUNCMDg4MEYxMUUxODU4OUQ4M0REMkFGNTBBNCIgc3RSZWY6ZG9jdW1lbnRJRD0ieG1wLmRpZDo5QzY2OUNCMTg4MEYxMUUxODU4OUQ4M0REMkFGNTBBNCIvPiA8L3JkZjpEZXNjcmlwdGlvbj4gPC9yZGY6UkRGPiA8L3g6eG1wbWV0YT4gPD94cGFja2V0IGVuZD0iciI/PsQhXeAAAABfSURBVHjaYvz//z8DJYCRUgMYQAbAMBQIAvEqkBQWXI6sHqwHiwG70TTBxGaiWwjCTGgOUgJiF1J8wMRAIUA34B4Q76HUBelAfJYSA0CuMIEaRP8wGIkGMA54bgQIMACAmkXJi0hKJQAAAABJRU5ErkJggg==) no-repeat 10px center;
  text-decoration: none; }

h1 tt, h1 code {
  font-size: inherit; }

h2 tt, h2 code {
  font-size: inherit; }

h3 tt, h3 code {
  font-size: inherit; }

h4 tt, h4 code {
  font-size: inherit; }

h5 tt, h5 code {
  font-size: inherit; }

h6 tt, h6 code {
  font-size: inherit; }

h1 {
  font-size: 28px;
  color: black; }

h2 {
  font-size: 24px;
  border-bottom: 1px solid #cccccc;
  color: black; }

h3 {
  font-size: 18px; }

h4 {
  font-size: 16px; }

h5 {
  font-size: 14px; }

h6 {
  color: #777777;
  font-size: 14px; }

p, blockquote, ul, ol, dl, li, table, pre {
  margin: 15px 0; }

hr {
  background: transparent url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAYAAAAECAYAAACtBE5DAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAAyJpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvIiB4bWxuczp4bXBNTT0iaHR0cDovL25zLmFkb2JlLmNvbS94YXAvMS4wL21tLyIgeG1sbnM6c3RSZWY9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9zVHlwZS9SZXNvdXJjZVJlZiMiIHhtcDpDcmVhdG9yVG9vbD0iQWRvYmUgUGhvdG9zaG9wIENTNSBNYWNpbnRvc2giIHhtcE1NOkluc3RhbmNlSUQ9InhtcC5paWQ6OENDRjNBN0E2NTZBMTFFMEI3QjRBODM4NzJDMjlGNDgiIHhtcE1NOkRvY3VtZW50SUQ9InhtcC5kaWQ6OENDRjNBN0I2NTZBMTFFMEI3QjRBODM4NzJDMjlGNDgiPiA8eG1wTU06RGVyaXZlZEZyb20gc3RSZWY6aW5zdGFuY2VJRD0ieG1wLmlpZDo4Q0NGM0E3ODY1NkExMUUwQjdCNEE4Mzg3MkMyOUY0OCIgc3RSZWY6ZG9jdW1lbnRJRD0ieG1wLmRpZDo4Q0NGM0E3OTY1NkExMUUwQjdCNEE4Mzg3MkMyOUY0OCIvPiA8L3JkZjpEZXNjcmlwdGlvbj4gPC9yZGY6UkRGPiA8L3g6eG1wbWV0YT4gPD94cGFja2V0IGVuZD0iciI/PqqezsUAAAAfSURBVHjaYmRABcYwBiM2QSA4y4hNEKYDQxAEAAIMAHNGAzhkPOlYAAAAAElFTkSuQmCC) repeat-x 0 0;
  border: 0 none;
  color: #cccccc;
  height: 4px;
  padding: 0;
}

body > h2:first-child {
  margin-top: 0;
  padding-top: 0; }
body > h1:first-child {
  margin-top: 0;
  padding-top: 0; }
  body > h1:first-child + h2 {
    margin-top: 0;
    padding-top: 0; }
body > h3:first-child, body > h4:first-child, body > h5:first-child, body > h6:first-child {
  margin-top: 0;
  padding-top: 0; }

a:first-child h1, a:first-child h2, a:first-child h3, a:first-child h4, a:first-child h5, a:first-child h6 {
  margin-top: 0;
  padding-top: 0; }

h1 p, h2 p, h3 p, h4 p, h5 p, h6 p {
  margin-top: 0; }

li p.first {
  display: inline-block; }
li {
  margin: 0; }
ul, ol {
  padding-left: 30px; }

ul :first-child, ol :first-child {
  margin-top: 0; }

dl {
  padding: 0; }
  dl dt {
    font-size: 14px;
    font-weight: bold;
    font-style: italic;
    padding: 0;
    margin: 15px 0 5px; }
    dl dt:first-child {
      padding: 0; }
    dl dt > :first-child {
      margin-top: 0; }
    dl dt > :last-child {
      margin-bottom: 0; }
  dl dd {
    margin: 0 0 15px;
    padding: 0 15px; }
    dl dd > :first-child {
      margin-top: 0; }
    dl dd > :last-child {
      margin-bottom: 0; }

blockquote {
  border-left: 4px solid #dddddd;
  padding: 0 15px;
  color: #777777; }
  blockquote > :first-child {
    margin-top: 0; }
  blockquote > :last-child {
    margin-bottom: 0; }

table {
  padding: 0;border-collapse: collapse; }
  table tr {
    border-top: 1px solid #cccccc;
    background-color: white;
    margin: 0;
    padding: 0; }
    table tr:nth-child(2n) {
      background-color: #f8f8f8; }
    table tr th {
      font-weight: bold;
      border: 1px solid #cccccc;
      margin: 0;
      padding: 6px 13px; }
    table tr td {
      border: 1px solid #cccccc;
      margin: 0;
      padding: 6px 13px; }
    table tr th :first-child, table tr td :first-child {
      margin-top: 0; }
    table tr th :last-child, table tr td :last-child {
      margin-bottom: 0; }

img {
  max-width: 100%; }

span.frame {
  display: block;
  overflow: hidden; }
  span.frame > span {
    border: 1px solid #dddddd;
    display: block;
    float: left;
    overflow: hidden;
    margin: 13px 0 0;
    padding: 7px;
    width: auto; }
  span.frame span img {
    display: block;
    float: left; }
  span.frame span span {
    clear: both;
    color: #333333;
    display: block;
    padding: 5px 0 0; }
span.align-center {
  display: block;
  overflow: hidden;
  clear: both; }
  span.align-center > span {
    display: block;
    overflow: hidden;
    margin: 13px auto 0;
    text-align: center; }
  span.align-center span img {
    margin: 0 auto;
    text-align: center; }
span.align-right {
  display: block;
  overflow: hidden;
  clear: both; }
  span.align-right > span {
    display: block;
    overflow: hidden;
    margin: 13px 0 0;
    text-align: right; }
  span.align-right span img {
    margin: 0;
    text-align: right; }
span.float-left {
  display: block;
  margin-right: 13px;
  overflow: hidden;
  float: left; }
  span.float-left span {
    margin: 13px 0 0; }
span.float-right {
  display: block;
  margin-left: 13px;
  overflow: hidden;
  float: right; }
  span.float-right > span {
    display: block;
    overflow: hidden;
    margin: 13px auto 0;
    text-align: right; }

code, tt {
  margin: 0 2px;
  padding: 0 5px;
  white-space: nowrap;
  border: 1px solid #eaeaea;
  background-color: #f8f8f8;
  border-radius: 3px; }

pre code {
  margin: 0;
  padding: 0;
  white-space: pre;
  border: none;
  background: transparent; }

.highlight pre {
  background-color: #f8f8f8;
  border: 1px solid #cccccc;
  font-size: 13px;
  line-height: 19px;
  overflow: auto;
  padding: 6px 10px;
  border-radius: 3px; }

pre {
  background-color: #f8f8f8;
  border: 1px solid #cccccc;
  font-size: 13px;
  line-height: 19px;
  overflow: auto;
  padding: 6px 10px;
  border-radius: 3px; }
  pre code, pre tt {
    background-color: transparent;
    border: none; }

sup {
    font-size: 0.83em;
    vertical-align: super;
    line-height: 0;
}

kbd {
  display: inline-block;
  padding: 3px 5px;
  font-size: 11px;
  line-height: 10px;
  color: #555;
  vertical-align: middle;
  background-color: #fcfcfc;
  border: solid 1px #ccc;
  border-bottom-color: #bbb;
  border-radius: 3px;
  box-shadow: inset 0 -1px 0 #bbb
}

* {
	-webkit-print-color-adjust: exact;
}
@media screen and (min-width: 914px) {
    body {
        width: 854px;
        margin:0 auto;
    }
}
@media print {
	table, pre {
		page-break-inside: avoid;
	}
	pre {
		word-wrap: break-word;
	}
  body {
    padding: 2cm; 
  }
}
</style>

<style type="text/css">
/**
 * prism.js default theme for JavaScript, CSS and HTML
 * Based on dabblet (http://dabblet.com)
 * @author Lea Verou
 */

code[class*="language-"],
pre[class*="language-"] {
	color: black;
	background: none;
	text-shadow: 0 1px white;
	font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
	text-align: left;
	white-space: pre;
	word-spacing: normal;
	word-break: normal;
	word-wrap: normal;
	line-height: 1.5;

	-moz-tab-size: 4;
	-o-tab-size: 4;
	tab-size: 4;

	-webkit-hyphens: none;
	-moz-hyphens: none;
	-ms-hyphens: none;
	hyphens: none;
}

pre[class*="language-"]::-moz-selection, pre[class*="language-"] ::-moz-selection,
code[class*="language-"]::-moz-selection, code[class*="language-"] ::-moz-selection {
	text-shadow: none;
	background: #b3d4fc;
}

pre[class*="language-"]::selection, pre[class*="language-"] ::selection,
code[class*="language-"]::selection, code[class*="language-"] ::selection {
	text-shadow: none;
	background: #b3d4fc;
}

@media print {
	code[class*="language-"],
	pre[class*="language-"] {
		text-shadow: none;
	}
}

/* Code blocks */
pre[class*="language-"] {
	padding: 1em;
	margin: .5em 0;
	overflow: auto;
}

:not(pre) > code[class*="language-"],
pre[class*="language-"] {
	background: #f5f2f0;
}

/* Inline code */
:not(pre) > code[class*="language-"] {
	padding: .1em;
	border-radius: .3em;
	white-space: normal;
}

.token.comment,
.token.prolog,
.token.doctype,
.token.cdata {
	color: slategray;
}

.token.punctuation {
	color: #999;
}

.namespace {
	opacity: .7;
}

.token.property,
.token.tag,
.token.boolean,
.token.number,
.token.constant,
.token.symbol,
.token.deleted {
	color: #905;
}

.token.selector,
.token.attr-name,
.token.string,
.token.char,
.token.builtin,
.token.inserted {
	color: #690;
}

.token.operator,
.token.entity,
.token.url,
.language-css .token.string,
.style .token.string {
	color: #a67f59;
	background: hsla(0, 0%, 100%, .5);
}

.token.atrule,
.token.attr-value,
.token.keyword {
	color: #07a;
}

.token.function {
	color: #DD4A68;
}

.token.regex,
.token.important,
.token.variable {
	color: #e90;
}

.token.important,
.token.bold {
	font-weight: bold;
}
.token.italic {
	font-style: italic;
}

.token.entity {
	cursor: help;
}
</style>

<style type="text/css">
pre.line-numbers {
	position: relative;
	padding-left: 3.8em;
	counter-reset: linenumber;
}

pre.line-numbers > code {
	position: relative;
}

.line-numbers .line-numbers-rows {
	position: absolute;
	pointer-events: none;
	top: 0;
	font-size: 100%;
	left: -3.8em;
	width: 3em; /* works for line-numbers below 1000 lines */
	letter-spacing: -1px;
	border-right: 1px solid #999;

	-webkit-user-select: none;
	-moz-user-select: none;
	-ms-user-select: none;
	user-select: none;

}

	.line-numbers-rows > span {
		pointer-events: none;
		display: block;
		counter-increment: linenumber;
	}

		.line-numbers-rows > span:before {
			content: counter(linenumber);
			color: #999;
			display: block;
			padding-right: 0.8em;
			text-align: right;
		}
</style>


</head>

<body>

<h1 id="toc_0">Real world problem: Predict rating given product reviews on Amazon</h1>

<script src="https://code.jquery.com/jquery-3.6.0.min.js" ></script>

<script src="../toc.js" ></script>

<div id='toc'></div>

<p>It is based on amazon food review data.</p>

<h2 id="toc_1">Text Preprocessing</h2>

<ol>
<li>Removing stop-words.</li>
<li>Make all words lowercase.</li>
<li>Stemming.</li>
<li>Lemmitization.</li>
<li>Tokenization.</li>
</ol>

<p>We can use fields &#39;Text&#39; and &#39;Summary&#39; for better analysis.</p>

<p>From Linear Algebra, we know that, if we have any input as vector, we can leverage all the mathematical learned in linear algebra.</p>

<p>How to convert simple text, words and sentences to numerical vectors?</p>

<p>Convert <strong>Review</strong> text to <strong>d-dim</strong> vector. Then we can draw <strong>hyperplane</strong> to do the classification.</p>

<p><img src="./1%20Real%20world%20problem-%20Predict%20rating%20given%20product%20reviews%20on%20Amazon/Screen%20Shot%202021-06-01%20at%208.42.23%20PM.png" alt=""></p>

<p>Text \(\rightarrow\) \(d\)-dim vector, the <strong>rules</strong> are</p>

<blockquote>
<p>Suppose reviews \(r_1\), \(r_2\), \(r_3\) has vectors \(v_1\), \(v_2\), \(v_3\) correspondingly. If \(r_1\) and \(r_2\) are more similar <strong>semantically</strong> than \(r_1\) and \(r_3\), then the distance between \(v_1\) and \(v_2\) should be less than distance between \(v_1\) and \(v_3\). It means that similar points are closer.<br>
(i.e.) If sem(\(r_1\), \(r_2\)) &gt; sem(\(r_1\), \(r_3\)), then dist(\(v_1\), \(v_2\)) &lt; dist(\(v_1\), \(v_3\))</p>
</blockquote>

<p>Find {Text \(\rightarrow\) \(d\)-dim vector}, such that similar text must be closer geometrically.</p>

<p>Each review is a <strong>document</strong>. And collection of documents is called <strong>corpus</strong>.</p>

<p>Such techniques are<br>
1. Bag of Words (BoW)<br>
2. tf-idf<br>
3. Word2Vec</p>

<h2 id="toc_2">Bag of Words (BoW)</h2>

<p>Lets<br>
\(r_1\) : This pasta is very tasty and affordable<br>
\(r_2\) : This pasta is not tasty and is affordable<br>
\(r_3\) : This pasta is delicious and cheap<br>
\(r_4\) : Pasta is tasty and pasta tastes good</p>

<p>Steps :<br>
1. Constructing a <strong>dictionary</strong> (set of all the words in your reviews). \(d\)-unique words across all reviews or document.<br>
2. Each word is a different dimension, with \(d\) dimensions for a document. For each review, we&#39;ll construct a vector with \(d\) dimension with value as the <strong>count of the corresponding words</strong>. Let&#39;s say for \(r_1\), the value is <strong>1</strong> for the index of the words <q>this</q>, <q>pasta</q>, <q>is</q>, <q>very</q>, <q>tasty</q>, <q>and</q> &amp; <q>affordable</q> and it&#39;ll be available in \(v_1\). \(v_1\) will be very sparse with most of the elements are <em>0</em>.</p>

<p>Now, we converted <strong>text</strong> to <strong>vector</strong>. If 2 documents are similar semantically, the resultant vectors must be closer.</p>

<p><img src="./1%20Real%20world%20problem-%20Predict%20rating%20given%20product%20reviews%20on%20Amazon/Screen%20Shot%202021-06-01%20at%209.19.32%20PM.png" alt=""></p>

<p><u>Correction:</u> <strong>is</strong> will occur only once. And the count will be put in the cell. So distance varies.</p>

<p>The length is very close. But their meanings are extremely opposite.</p>

<p><strong>Binary/Boolean BoW :</strong> We&#39;ll put <strong>1</strong> if a number occurs else <strong>0</strong>. It&#39;ll return the <strong>no. of differing words</strong>.</p>

<p>Also, the trivial words like <strong>is</strong>, <strong>this</strong>, <strong>and</strong> etc are not important. They are called <strong>stop</strong> words.</p>

<p>After removing those, the \(v_i\) vectors will be small and more meaningful. Also we are throwing away some information. In english, <strong>not</strong> is a stop word. But removing it will cause problem.</p>

<h3 id="toc_3">Stemming</h3>

<p><strong>Tasty</strong>, <strong>tasting</strong>, <strong>tasteful</strong> - instead of having them as 3 words in BoW, convert each of these words into their common form <strong>tast</strong> and replace those words with this. So many algorithms to do this like <em>Porter stemmer</em> and <em>snow ball stemmer</em>.</p>

<h3 id="toc_4">Lemmitization</h3>

<p>Lemmatisation is the algorithmic process of determining the lemma of a word based on its intended meaning. Unlike stemming, lemmatization depends on correctly identifying the intended part of speech and meaning of a word in a sentence, as well as within the larger context surrounding that sentence, such as neighboring sentences or even an entire document. <strong>Tasty</strong>, <strong>tasting</strong>, <strong>tasteful</strong> will be converted to <strong>taste</strong></p>

<p>Read : <a href="https://blog.bitext.com/what-is-the-difference-between-stemming-and-lemmatization/">https://blog.bitext.com/what-is-the-difference-between-stemming-and-lemmatization/</a></p>

<h3 id="toc_5">Tokenization</h3>

<p>Breaking a sentence into words. We use <q><strong>space</strong></q> to separate sentence into words. But in case of word <q>New York</q>, it is not possible as it is <strong>language dependent</strong> and <strong>context dependent</strong>. <strong>Tasty</strong> and <strong>delicious</strong> are synonyms.<br>
We can make use of <strong>Word2Vec</strong> to achieve this synonyms.</p>

<p><strong>BoW</strong> doesn&#39;t take the semantic meaning into the consideration.</p>

<h3 id="toc_6">uni-gram, bi-gram, n-grams.</h3>

<p>\(r_1\) : This pasta is very tasty and affordable<br>
\(r_2\) : This pasta is not tasty and is affordable</p>

<p>After removing stop words (this, is, not, very, and), \(v_1\) and \(v_2\) are exactly the same. Then we&#39;ll consider the both are similar but in reality, they are not.</p>

<p>We can fix it by &#39;bi-gram&#39; or &#39;tri-gram&#39; as unigram BoW discard the sequence of information. In <strong>unigram</strong>, each word will be considered a dimension. In <strong>bi-gram</strong>, pair of words will form a dimension. So &#39;This pasta&#39;, &#39;pasta is&#39;, &#39;is very&#39;, &#39;very tasty&#39; and so on will form dimensions. Inn <strong>tri-gram</strong>, take 3 consecutive words as one dimension and it follows for <strong>n-gram</strong>. Other than, unigram, others will retain some of the sequential information.</p>

<p>If <strong>there are repeated words</strong>, the dimensions in <code>#n-gram</code> \(\geq\) <code>#tri-gram</code> \(\geq\) <code>#bi-gram</code> \(\geq\) <code>#uni-gram</code>.</p>

<p>Eg :<br>
<strong>horse is a horse, of course, of course. accept  it</strong><br>
Here there are repeated words<br>
unigrams --&gt; horse, of, course, a, is, it, accept --&gt; #7<br>
bigrams --&gt; horse is, is a , a horse, horse of, of course, course of, course accept, accept it --&gt; #8<br>
trigrams --&gt; horse is a, is a horse, a horse of, horse of course, of course of, course of course, of course accept, course accept it --&gt; #8</p>

<h2 id="toc_7">tf-idf (term frequency- inverse document frequency) BoW</h2>

<p>This method doesn&#39;t take the semantic meaning into the consideration.</p>

<p>Let&#39;s assume we have \(N\) documents/reviews.</p>

<p><strong>Term frequency</strong> of word \(W_i\) occuring in the document/review \(r_j\) is<br>
\(TF=\frac{\#\ of\ times\ W_i\ occurs\ in\ r_j}{total\ \#\ of\ words\ in\ r_j}\)</p>

<p>\(D_c\) is set of all the below reviews. It is <strong>corpus</strong><br>
\(r_1\) is \(W_1\), \(W_2\), \(W_3\), \(W_2\), \(W_5\)<br>
\(r_2\) is \(W_1\), \(W_2\), \(W_4\), \(W_3\), \(W_5\), \(W_6\)<br>
...<br>
\(r_N\)</p>

<p>\(TF(W_2,r_1)=\frac{2}{5}\)</p>

<p>\(0\ \leq\ TF\ \leq\ 1\), so it is like probability of finding a word in the document.</p>

<p><strong>IDF</strong> : If a word occurs in many documents, then IDF is low. If a word is a rare or low frequency, then IDF is high.</p>

<p>\(IDF(W_i, D_c)\) = \(log(\frac{N}{n_i})\) where \(n_i\) is the no of documents which has the word \(W_i\)</p>

<p>\(n_i\ \leq\ N\ \Longrightarrow\ \frac{N}{n_i}\geq1\ \Longrightarrow\ IDF=log(\frac{N}{n_i})\geq0\)<br>
As \(n_i\) increases, \(\frac{N}{n_i}\) and \(log(\frac{N}{n_i})\) both decreases.</p>

<p><strong>It means that if a word occurs more frequently in the corpus, then the \(IDF\) value is smaller.</strong><br>
(i.e.) if \(n_i\uparrow\), \(IDF\downarrow\). Also if \(n_i\downarrow,IDF\uparrow\)</p>

<p>Usually in <strong>BoW</strong>, we&#39;ll fill the vector with the count of words.<br>
But now, we&#39;ll fill <strong>index</strong> corresponding to the word \(W_i\) with <strong>\(TF(W_i, r_j)*IDF(W_i,D_c)\)</strong> in the document \(r_j\). It gives more weightwage to the most frequent words in a document and less weightage to the common words in the corpus.</p>

<p><strong>More importance to the rarer word in the corpus and to the frequent word in a document.</strong></p>

<p>It still doesn&#39;t take into account the semmantic meaning of the word.</p>

<h3 id="toc_8">Why use log in IDF?</h3>

<p>As per Zipf&#39;s law, the common words in english will occur much more and rarer words will be less. It follows <strong>power-law</strong> distribution.<br>
<img src="./1%20Real%20world%20problem-%20Predict%20rating%20given%20product%20reviews%20on%20Amazon/Screen%20Shot%202021-06-07%20at%2011.06.23%20AM.png" alt=""><br>
<img src="./1%20Real%20world%20problem-%20Predict%20rating%20given%20product%20reviews%20on%20Amazon/Screen%20Shot%202021-06-07%20at%2011.07.53%20AM.png" alt=""><br>
Instead of giving weightage 1000 to the word <strong>civilization</strong>, we are just giving \(6.9\) .</p>

<p><code>Both BoW and tf-idf convert a full text/document into a sparse vector.</code></p>

<h2 id="toc_9">Word2Vec</h2>

<p>Takes the <strong>semantics into the consideration</strong>. It is the state of the art technique. It learns relationships automatically from raw text.</p>

<p>Given a <strong>word</strong>, it&#39;ll be converted to <strong>d-dimensional dense vector</strong> (typically d will be 100,200,300). Note that, here word is converted to a vector not a sentence or document.</p>

<p>Take 3 words <q>tasty</q>, <q>delicious</q>, <q>baseball</q>. As Word2Vec knows English, it&#39;ll return 3 vectors \(v_1,v_2\) and \(v_3\) respectively such that the distance between first 2 are closer and \(v_3\) is far.</p>

<ol>
<li>So, given any 2 semantically similar words, Word2Vec converts them to 2 vectors with minimal distance.</li>
<li>Relationships are satisfied. Take words man, woman, king and queen, their vectors \(v_{man}\), \(v_{woman}\), \(v_{king}\) and \(v_{queen}\), then (\(v_{man}\)-\(v_{woman}\)) vector is parallel to (\(v_{king}\)-\(v_{queen}\)) vector. Same applicable for country-capital, verb tense, gender and so on. </li>
</ol>

<p><img src="./1%20Real%20world%20problem-%20Predict%20rating%20given%20product%20reviews%20on%20Amazon/Screen%20Shot%202021-06-07%20at%2011.45.52%20AM.png" alt=""></p>

<p><img src="./1%20Real%20world%20problem-%20Predict%20rating%20given%20product%20reviews%20on%20Amazon/Screen%20Shot%202021-06-07%20at%2011.50.18%20AM.png" alt=""></p>

<p>As corpus size increases, the d-dimensions will also increase with information rich vectors.</p>

<p>Intuitively, it works like if the neighborhood of the word \(W_i\) is similar to the word \(W_j\), then \(v_i\) is similar to \(v_j\)<br>
(i.e) \(N(W_i)\approx N(W_j)\)  \(\Longrightarrow\) \(v_i\approx v_j\)</p>

<h3 id="toc_10">Using sentences, how to convert using word2vec?</h3>

<p>Using Avg-Word2Vec, tf-idf weighted Word2Vec weighing strategies.</p>

<p><strong><u>Avg-Word2Vec</u></strong><br>
\(r_1\) : \(W_1\) \(W_2\) \(W_1\) \(W_3\) \(W_4\) \(W_5\) </p>

<p>Then \(r_1\rightarrow v_1\) is \(\frac{W2V(W_1)+W2V(W_2)+W2V(W_1)...W2V(W_5)}{n_1}\)</p>

<p>This is averaging all the vectors. It is not perfect. It is well enough.</p>

<p><strong><u>tf-idf weighted Word2Vec</u></strong></p>

<p>Compute the <strong>tf_idf</strong> for \(r_1\), then multiply each corresponding value with the \(W2V(W_i)\)</p>

<p><img src="./1%20Real%20world%20problem-%20Predict%20rating%20given%20product%20reviews%20on%20Amazon/Screen%20Shot%202021-06-07%20at%2012.12.55%20PM.png" alt=""></p>

<p>\(tfidf_w2v(r_1)=\frac{\sum_{i:words}t_i*w2v(W_i)}{\sum_{i:words}t_i}\)</p>



<script type="text/javascript">
var _self="undefined"!=typeof window?window:"undefined"!=typeof WorkerGlobalScope&&self instanceof WorkerGlobalScope?self:{},Prism=function(){var e=/\blang(?:uage)?-(\w+)\b/i,t=0,n=_self.Prism={util:{encode:function(e){return e instanceof a?new a(e.type,n.util.encode(e.content),e.alias):"Array"===n.util.type(e)?e.map(n.util.encode):e.replace(/&/g,"&amp;").replace(/</g,"&lt;").replace(/\u00a0/g," ")},type:function(e){return Object.prototype.toString.call(e).match(/\[object (\w+)\]/)[1]},objId:function(e){return e.__id||Object.defineProperty(e,"__id",{value:++t}),e.__id},clone:function(e){var t=n.util.type(e);switch(t){case"Object":var a={};for(var r in e)e.hasOwnProperty(r)&&(a[r]=n.util.clone(e[r]));return a;case"Array":return e.map&&e.map(function(e){return n.util.clone(e)})}return e}},languages:{extend:function(e,t){var a=n.util.clone(n.languages[e]);for(var r in t)a[r]=t[r];return a},insertBefore:function(e,t,a,r){r=r||n.languages;var l=r[e];if(2==arguments.length){a=arguments[1];for(var i in a)a.hasOwnProperty(i)&&(l[i]=a[i]);return l}var o={};for(var s in l)if(l.hasOwnProperty(s)){if(s==t)for(var i in a)a.hasOwnProperty(i)&&(o[i]=a[i]);o[s]=l[s]}return n.languages.DFS(n.languages,function(t,n){n===r[e]&&t!=e&&(this[t]=o)}),r[e]=o},DFS:function(e,t,a,r){r=r||{};for(var l in e)e.hasOwnProperty(l)&&(t.call(e,l,e[l],a||l),"Object"!==n.util.type(e[l])||r[n.util.objId(e[l])]?"Array"!==n.util.type(e[l])||r[n.util.objId(e[l])]||(r[n.util.objId(e[l])]=!0,n.languages.DFS(e[l],t,l,r)):(r[n.util.objId(e[l])]=!0,n.languages.DFS(e[l],t,null,r)))}},plugins:{},highlightAll:function(e,t){var a={callback:t,selector:'code[class*="language-"], [class*="language-"] code, code[class*="lang-"], [class*="lang-"] code'};n.hooks.run("before-highlightall",a);for(var r,l=a.elements||document.querySelectorAll(a.selector),i=0;r=l[i++];)n.highlightElement(r,e===!0,a.callback)},highlightElement:function(t,a,r){for(var l,i,o=t;o&&!e.test(o.className);)o=o.parentNode;o&&(l=(o.className.match(e)||[,""])[1],i=n.languages[l]),t.className=t.className.replace(e,"").replace(/\s+/g," ")+" language-"+l,o=t.parentNode,/pre/i.test(o.nodeName)&&(o.className=o.className.replace(e,"").replace(/\s+/g," ")+" language-"+l);var s=t.textContent,u={element:t,language:l,grammar:i,code:s};if(!s||!i)return n.hooks.run("complete",u),void 0;if(n.hooks.run("before-highlight",u),a&&_self.Worker){var c=new Worker(n.filename);c.onmessage=function(e){u.highlightedCode=e.data,n.hooks.run("before-insert",u),u.element.innerHTML=u.highlightedCode,r&&r.call(u.element),n.hooks.run("after-highlight",u),n.hooks.run("complete",u)},c.postMessage(JSON.stringify({language:u.language,code:u.code,immediateClose:!0}))}else u.highlightedCode=n.highlight(u.code,u.grammar,u.language),n.hooks.run("before-insert",u),u.element.innerHTML=u.highlightedCode,r&&r.call(t),n.hooks.run("after-highlight",u),n.hooks.run("complete",u)},highlight:function(e,t,r){var l=n.tokenize(e,t);return a.stringify(n.util.encode(l),r)},tokenize:function(e,t){var a=n.Token,r=[e],l=t.rest;if(l){for(var i in l)t[i]=l[i];delete t.rest}e:for(var i in t)if(t.hasOwnProperty(i)&&t[i]){var o=t[i];o="Array"===n.util.type(o)?o:[o];for(var s=0;s<o.length;++s){var u=o[s],c=u.inside,g=!!u.lookbehind,h=!!u.greedy,f=0,d=u.alias;u=u.pattern||u;for(var p=0;p<r.length;p++){var m=r[p];if(r.length>e.length)break e;if(!(m instanceof a)){u.lastIndex=0;var y=u.exec(m),v=1;if(!y&&h&&p!=r.length-1){var b=r[p+1].matchedStr||r[p+1],k=m+b;if(p<r.length-2&&(k+=r[p+2].matchedStr||r[p+2]),u.lastIndex=0,y=u.exec(k),!y)continue;var w=y.index+(g?y[1].length:0);if(w>=m.length)continue;var _=y.index+y[0].length,P=m.length+b.length;if(v=3,P>=_){if(r[p+1].greedy)continue;v=2,k=k.slice(0,P)}m=k}if(y){g&&(f=y[1].length);var w=y.index+f,y=y[0].slice(f),_=w+y.length,S=m.slice(0,w),O=m.slice(_),j=[p,v];S&&j.push(S);var A=new a(i,c?n.tokenize(y,c):y,d,y,h);j.push(A),O&&j.push(O),Array.prototype.splice.apply(r,j)}}}}}return r},hooks:{all:{},add:function(e,t){var a=n.hooks.all;a[e]=a[e]||[],a[e].push(t)},run:function(e,t){var a=n.hooks.all[e];if(a&&a.length)for(var r,l=0;r=a[l++];)r(t)}}},a=n.Token=function(e,t,n,a,r){this.type=e,this.content=t,this.alias=n,this.matchedStr=a||null,this.greedy=!!r};if(a.stringify=function(e,t,r){if("string"==typeof e)return e;if("Array"===n.util.type(e))return e.map(function(n){return a.stringify(n,t,e)}).join("");var l={type:e.type,content:a.stringify(e.content,t,r),tag:"span",classes:["token",e.type],attributes:{},language:t,parent:r};if("comment"==l.type&&(l.attributes.spellcheck="true"),e.alias){var i="Array"===n.util.type(e.alias)?e.alias:[e.alias];Array.prototype.push.apply(l.classes,i)}n.hooks.run("wrap",l);var o="";for(var s in l.attributes)o+=(o?" ":"")+s+'="'+(l.attributes[s]||"")+'"';return"<"+l.tag+' class="'+l.classes.join(" ")+'" '+o+">"+l.content+"</"+l.tag+">"},!_self.document)return _self.addEventListener?(_self.addEventListener("message",function(e){var t=JSON.parse(e.data),a=t.language,r=t.code,l=t.immediateClose;_self.postMessage(n.highlight(r,n.languages[a],a)),l&&_self.close()},!1),_self.Prism):_self.Prism;var r=document.currentScript||[].slice.call(document.getElementsByTagName("script")).pop();return r&&(n.filename=r.src,document.addEventListener&&!r.hasAttribute("data-manual")&&document.addEventListener("DOMContentLoaded",n.highlightAll)),_self.Prism}();"undefined"!=typeof module&&module.exports&&(module.exports=Prism),"undefined"!=typeof global&&(global.Prism=Prism);
</script>

<script type="text/javascript">
!function(){"undefined"!=typeof self&&self.Prism&&self.document&&Prism.hooks.add("complete",function(e){if(e.code){var t=e.element.parentNode,s=/\s*\bline-numbers\b\s*/;if(t&&/pre/i.test(t.nodeName)&&(s.test(t.className)||s.test(e.element.className))&&!e.element.querySelector(".line-numbers-rows")){s.test(e.element.className)&&(e.element.className=e.element.className.replace(s,"")),s.test(t.className)||(t.className+=" line-numbers");var n,a=e.code.match(/\n(?!$)/g),l=a?a.length+1:1,m=new Array(l+1);m=m.join("<span></span>"),n=document.createElement("span"),n.className="line-numbers-rows",n.innerHTML=m,t.hasAttribute("data-start")&&(t.style.counterReset="linenumber "+(parseInt(t.getAttribute("data-start"),10)-1)),e.element.appendChild(n)}}})}();
</script>

<script type="text/x-mathjax-config">
(function () {

MathJax.Hub.Config({
	'showProcessingMessages': false,
	'messageStyle': 'none'
});

if (typeof MathJaxListener !== 'undefined') {
	MathJax.Hub.Register.StartupHook('End', function () {
		MathJaxListener.invokeCallbackForKey_('End');
	});
}

})();
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


</body>

</html>
