<!DOCTYPE html>
<html>

<head>

<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<title>1 Support Vector Machines (SVM)</title>


<style type="text/css">
body {
  font-family: Helvetica, arial, sans-serif;
  font-size: 14px;
  line-height: 1.6;
  padding-top: 10px;
  padding-bottom: 10px;
  background-color: white;
  padding: 30px; }

body > *:first-child {
  margin-top: 0 !important; }
body > *:last-child {
  margin-bottom: 0 !important; }

a {
  color: #4183C4; }
a.absent {
  color: #cc0000; }
a.anchor {
  display: block;
  padding-left: 30px;
  margin-left: -30px;
  cursor: pointer;
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0; }

h1, h2, h3, h4, h5, h6 {
  margin: 20px 0 10px;
  padding: 0;
  font-weight: bold;
  -webkit-font-smoothing: antialiased;
  cursor: text;
  position: relative; }

h1:hover a.anchor, h2:hover a.anchor, h3:hover a.anchor, h4:hover a.anchor, h5:hover a.anchor, h6:hover a.anchor {
  background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA09pVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMy1jMDExIDY2LjE0NTY2MSwgMjAxMi8wMi8wNi0xNDo1NjoyNyAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvIiB4bWxuczp4bXBNTT0iaHR0cDovL25zLmFkb2JlLmNvbS94YXAvMS4wL21tLyIgeG1sbnM6c3RSZWY9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9zVHlwZS9SZXNvdXJjZVJlZiMiIHhtcDpDcmVhdG9yVG9vbD0iQWRvYmUgUGhvdG9zaG9wIENTNiAoMTMuMCAyMDEyMDMwNS5tLjQxNSAyMDEyLzAzLzA1OjIxOjAwOjAwKSAgKE1hY2ludG9zaCkiIHhtcE1NOkluc3RhbmNlSUQ9InhtcC5paWQ6OUM2NjlDQjI4ODBGMTFFMTg1ODlEODNERDJBRjUwQTQiIHhtcE1NOkRvY3VtZW50SUQ9InhtcC5kaWQ6OUM2NjlDQjM4ODBGMTFFMTg1ODlEODNERDJBRjUwQTQiPiA8eG1wTU06RGVyaXZlZEZyb20gc3RSZWY6aW5zdGFuY2VJRD0ieG1wLmlpZDo5QzY2OUNCMDg4MEYxMUUxODU4OUQ4M0REMkFGNTBBNCIgc3RSZWY6ZG9jdW1lbnRJRD0ieG1wLmRpZDo5QzY2OUNCMTg4MEYxMUUxODU4OUQ4M0REMkFGNTBBNCIvPiA8L3JkZjpEZXNjcmlwdGlvbj4gPC9yZGY6UkRGPiA8L3g6eG1wbWV0YT4gPD94cGFja2V0IGVuZD0iciI/PsQhXeAAAABfSURBVHjaYvz//z8DJYCRUgMYQAbAMBQIAvEqkBQWXI6sHqwHiwG70TTBxGaiWwjCTGgOUgJiF1J8wMRAIUA34B4Q76HUBelAfJYSA0CuMIEaRP8wGIkGMA54bgQIMACAmkXJi0hKJQAAAABJRU5ErkJggg==) no-repeat 10px center;
  text-decoration: none; }

h1 tt, h1 code {
  font-size: inherit; }

h2 tt, h2 code {
  font-size: inherit; }

h3 tt, h3 code {
  font-size: inherit; }

h4 tt, h4 code {
  font-size: inherit; }

h5 tt, h5 code {
  font-size: inherit; }

h6 tt, h6 code {
  font-size: inherit; }

h1 {
  font-size: 28px;
  color: black; }

h2 {
  font-size: 24px;
  border-bottom: 1px solid #cccccc;
  color: black; }

h3 {
  font-size: 18px; }

h4 {
  font-size: 16px; }

h5 {
  font-size: 14px; }

h6 {
  color: #777777;
  font-size: 14px; }

p, blockquote, ul, ol, dl, li, table, pre {
  margin: 15px 0; }

hr {
  background: transparent url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAYAAAAECAYAAACtBE5DAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAAyJpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvIiB4bWxuczp4bXBNTT0iaHR0cDovL25zLmFkb2JlLmNvbS94YXAvMS4wL21tLyIgeG1sbnM6c3RSZWY9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9zVHlwZS9SZXNvdXJjZVJlZiMiIHhtcDpDcmVhdG9yVG9vbD0iQWRvYmUgUGhvdG9zaG9wIENTNSBNYWNpbnRvc2giIHhtcE1NOkluc3RhbmNlSUQ9InhtcC5paWQ6OENDRjNBN0E2NTZBMTFFMEI3QjRBODM4NzJDMjlGNDgiIHhtcE1NOkRvY3VtZW50SUQ9InhtcC5kaWQ6OENDRjNBN0I2NTZBMTFFMEI3QjRBODM4NzJDMjlGNDgiPiA8eG1wTU06RGVyaXZlZEZyb20gc3RSZWY6aW5zdGFuY2VJRD0ieG1wLmlpZDo4Q0NGM0E3ODY1NkExMUUwQjdCNEE4Mzg3MkMyOUY0OCIgc3RSZWY6ZG9jdW1lbnRJRD0ieG1wLmRpZDo4Q0NGM0E3OTY1NkExMUUwQjdCNEE4Mzg3MkMyOUY0OCIvPiA8L3JkZjpEZXNjcmlwdGlvbj4gPC9yZGY6UkRGPiA8L3g6eG1wbWV0YT4gPD94cGFja2V0IGVuZD0iciI/PqqezsUAAAAfSURBVHjaYmRABcYwBiM2QSA4y4hNEKYDQxAEAAIMAHNGAzhkPOlYAAAAAElFTkSuQmCC) repeat-x 0 0;
  border: 0 none;
  color: #cccccc;
  height: 4px;
  padding: 0;
}

body > h2:first-child {
  margin-top: 0;
  padding-top: 0; }
body > h1:first-child {
  margin-top: 0;
  padding-top: 0; }
  body > h1:first-child + h2 {
    margin-top: 0;
    padding-top: 0; }
body > h3:first-child, body > h4:first-child, body > h5:first-child, body > h6:first-child {
  margin-top: 0;
  padding-top: 0; }

a:first-child h1, a:first-child h2, a:first-child h3, a:first-child h4, a:first-child h5, a:first-child h6 {
  margin-top: 0;
  padding-top: 0; }

h1 p, h2 p, h3 p, h4 p, h5 p, h6 p {
  margin-top: 0; }

li p.first {
  display: inline-block; }
li {
  margin: 0; }
ul, ol {
  padding-left: 30px; }

ul :first-child, ol :first-child {
  margin-top: 0; }

dl {
  padding: 0; }
  dl dt {
    font-size: 14px;
    font-weight: bold;
    font-style: italic;
    padding: 0;
    margin: 15px 0 5px; }
    dl dt:first-child {
      padding: 0; }
    dl dt > :first-child {
      margin-top: 0; }
    dl dt > :last-child {
      margin-bottom: 0; }
  dl dd {
    margin: 0 0 15px;
    padding: 0 15px; }
    dl dd > :first-child {
      margin-top: 0; }
    dl dd > :last-child {
      margin-bottom: 0; }

blockquote {
  border-left: 4px solid #dddddd;
  padding: 0 15px;
  color: #777777; }
  blockquote > :first-child {
    margin-top: 0; }
  blockquote > :last-child {
    margin-bottom: 0; }

table {
  padding: 0;border-collapse: collapse; }
  table tr {
    border-top: 1px solid #cccccc;
    background-color: white;
    margin: 0;
    padding: 0; }
    table tr:nth-child(2n) {
      background-color: #f8f8f8; }
    table tr th {
      font-weight: bold;
      border: 1px solid #cccccc;
      margin: 0;
      padding: 6px 13px; }
    table tr td {
      border: 1px solid #cccccc;
      margin: 0;
      padding: 6px 13px; }
    table tr th :first-child, table tr td :first-child {
      margin-top: 0; }
    table tr th :last-child, table tr td :last-child {
      margin-bottom: 0; }

img {
  max-width: 100%; }

span.frame {
  display: block;
  overflow: hidden; }
  span.frame > span {
    border: 1px solid #dddddd;
    display: block;
    float: left;
    overflow: hidden;
    margin: 13px 0 0;
    padding: 7px;
    width: auto; }
  span.frame span img {
    display: block;
    float: left; }
  span.frame span span {
    clear: both;
    color: #333333;
    display: block;
    padding: 5px 0 0; }
span.align-center {
  display: block;
  overflow: hidden;
  clear: both; }
  span.align-center > span {
    display: block;
    overflow: hidden;
    margin: 13px auto 0;
    text-align: center; }
  span.align-center span img {
    margin: 0 auto;
    text-align: center; }
span.align-right {
  display: block;
  overflow: hidden;
  clear: both; }
  span.align-right > span {
    display: block;
    overflow: hidden;
    margin: 13px 0 0;
    text-align: right; }
  span.align-right span img {
    margin: 0;
    text-align: right; }
span.float-left {
  display: block;
  margin-right: 13px;
  overflow: hidden;
  float: left; }
  span.float-left span {
    margin: 13px 0 0; }
span.float-right {
  display: block;
  margin-left: 13px;
  overflow: hidden;
  float: right; }
  span.float-right > span {
    display: block;
    overflow: hidden;
    margin: 13px auto 0;
    text-align: right; }

code, tt {
  margin: 0 2px;
  padding: 0 5px;
  white-space: nowrap;
  border: 1px solid #eaeaea;
  background-color: #f8f8f8;
  border-radius: 3px; }

pre code {
  margin: 0;
  padding: 0;
  white-space: pre;
  border: none;
  background: transparent; }

.highlight pre {
  background-color: #f8f8f8;
  border: 1px solid #cccccc;
  font-size: 13px;
  line-height: 19px;
  overflow: auto;
  padding: 6px 10px;
  border-radius: 3px; }

pre {
  background-color: #f8f8f8;
  border: 1px solid #cccccc;
  font-size: 13px;
  line-height: 19px;
  overflow: auto;
  padding: 6px 10px;
  border-radius: 3px; }
  pre code, pre tt {
    background-color: transparent;
    border: none; }

sup {
    font-size: 0.83em;
    vertical-align: super;
    line-height: 0;
}

kbd {
  display: inline-block;
  padding: 3px 5px;
  font-size: 11px;
  line-height: 10px;
  color: #555;
  vertical-align: middle;
  background-color: #fcfcfc;
  border: solid 1px #ccc;
  border-bottom-color: #bbb;
  border-radius: 3px;
  box-shadow: inset 0 -1px 0 #bbb
}

* {
	-webkit-print-color-adjust: exact;
}
@media screen and (min-width: 914px) {
    body {
        width: 854px;
        margin:0 auto;
    }
}
@media print {
	table, pre {
		page-break-inside: avoid;
	}
	pre {
		word-wrap: break-word;
	}
  body {
    padding: 2cm; 
  }
}
</style>

<style type="text/css">
/**
 * prism.js default theme for JavaScript, CSS and HTML
 * Based on dabblet (http://dabblet.com)
 * @author Lea Verou
 */

code[class*="language-"],
pre[class*="language-"] {
	color: black;
	background: none;
	text-shadow: 0 1px white;
	font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
	text-align: left;
	white-space: pre;
	word-spacing: normal;
	word-break: normal;
	word-wrap: normal;
	line-height: 1.5;

	-moz-tab-size: 4;
	-o-tab-size: 4;
	tab-size: 4;

	-webkit-hyphens: none;
	-moz-hyphens: none;
	-ms-hyphens: none;
	hyphens: none;
}

pre[class*="language-"]::-moz-selection, pre[class*="language-"] ::-moz-selection,
code[class*="language-"]::-moz-selection, code[class*="language-"] ::-moz-selection {
	text-shadow: none;
	background: #b3d4fc;
}

pre[class*="language-"]::selection, pre[class*="language-"] ::selection,
code[class*="language-"]::selection, code[class*="language-"] ::selection {
	text-shadow: none;
	background: #b3d4fc;
}

@media print {
	code[class*="language-"],
	pre[class*="language-"] {
		text-shadow: none;
	}
}

/* Code blocks */
pre[class*="language-"] {
	padding: 1em;
	margin: .5em 0;
	overflow: auto;
}

:not(pre) > code[class*="language-"],
pre[class*="language-"] {
	background: #f5f2f0;
}

/* Inline code */
:not(pre) > code[class*="language-"] {
	padding: .1em;
	border-radius: .3em;
	white-space: normal;
}

.token.comment,
.token.prolog,
.token.doctype,
.token.cdata {
	color: slategray;
}

.token.punctuation {
	color: #999;
}

.namespace {
	opacity: .7;
}

.token.property,
.token.tag,
.token.boolean,
.token.number,
.token.constant,
.token.symbol,
.token.deleted {
	color: #905;
}

.token.selector,
.token.attr-name,
.token.string,
.token.char,
.token.builtin,
.token.inserted {
	color: #690;
}

.token.operator,
.token.entity,
.token.url,
.language-css .token.string,
.style .token.string {
	color: #a67f59;
	background: hsla(0, 0%, 100%, .5);
}

.token.atrule,
.token.attr-value,
.token.keyword {
	color: #07a;
}

.token.function {
	color: #DD4A68;
}

.token.regex,
.token.important,
.token.variable {
	color: #e90;
}

.token.important,
.token.bold {
	font-weight: bold;
}
.token.italic {
	font-style: italic;
}

.token.entity {
	cursor: help;
}
</style>

<style type="text/css">
pre.line-numbers {
	position: relative;
	padding-left: 3.8em;
	counter-reset: linenumber;
}

pre.line-numbers > code {
	position: relative;
}

.line-numbers .line-numbers-rows {
	position: absolute;
	pointer-events: none;
	top: 0;
	font-size: 100%;
	left: -3.8em;
	width: 3em; /* works for line-numbers below 1000 lines */
	letter-spacing: -1px;
	border-right: 1px solid #999;

	-webkit-user-select: none;
	-moz-user-select: none;
	-ms-user-select: none;
	user-select: none;

}

	.line-numbers-rows > span {
		pointer-events: none;
		display: block;
		counter-increment: linenumber;
	}

		.line-numbers-rows > span:before {
			content: counter(linenumber);
			color: #999;
			display: block;
			padding-right: 0.8em;
			text-align: right;
		}
</style>


</head>

<body>

<h1 id="toc_0">Support Vector Machines (SVM)</h1>

<script src="https://code.jquery.com/jquery-3.6.0.min.js" ></script>

<script src="../toc.js" ></script>

<div id='toc'></div>

<h3 id="toc_1">Good Reads:</h3>

<ol>
<li><a href="https://s3.amazonaws.com/ebooks.syncfusion.com/downloads/support_vector_machines_succinctly/support_vector_machines_succinctly.pdf?AWSAccessKeyId=AKIAWH6GYCX3445MQQ5X&amp;Expires=1626440437&amp;Signature=qFtL9LYj0YgPE12IZFrbAPI%2F2E0%3D">Alexandre KOWALCZYK Book</a></li>
<li><a href="https://www.svm-tutorial.com/2014/11/svm-understanding-math-part-2/">Alexandre KOWALCZYK Math Tutorial</a></li>
<li><a href="https://towardsdatascience.com/an-introduction-to-support-vector-regression-svr-a3ebc1672c2">SVR - Towards data Science</a></li>
</ol>

<p>Both <strong>classification</strong> (SVM-classification - <strong>SVC</strong>) and <strong>regression</strong> (SVM-Regression - <strong>SVR</strong>).</p>

<h2 id="toc_2">Geometric Intution</h2>

<p><img src="./1%20Support%20Vector%20Machines%20(SVM)/Screen%20Shot%202021-07-08%20at%205.17.50%20PM.png" alt=""><br>
<img src="./1%20Support%20Vector%20Machines%20(SVM)/Screen%20Shot%202021-07-08%20at%205.24.48%20PM.png" alt=""></p>

<p>We&#39;ll try to find a <strong>hyperplane</strong> \(\pi\) such that it separates the group of <strong>points as wide as possible</strong>. Considering the binary classification of \(+ve\) and \(-ve\) points, we can draw a line \(\pi\) in between the 2 groups such that when we draw a parallel line \(\pi^{+}\) (positive hyperplane) touching the first point of the +ve group and another line \(\pi^{-}\) (negative hyperplane) touching the first point of the -ve group. Then the <strong>margin</strong>, <strong>d</strong> is <strong>dist(</strong>\(\pi^+\)<strong>,</strong>\(\pi^-\)<strong>)</strong> and \(\pi\) is <strong>margin maximising hyperplane</strong>. The points through which the \(\pi^+\) and \(\pi^-\) passes are called <strong>support vectors (svs)</strong>.</p>

<p>So, SVM try to find a hyperplane that maximises the margin. This will <strong>minimize the error of misclassification</strong> and <strong>increases the accuracy</strong>.</p>

<h3 id="toc_3">Alternate Geometric Intuition</h3>

<p>First we&#39;ll draw convex-hull. It is a <strong>smallest convex polygon</strong> which covers the external points in a way that all the points are either inside the polygon or on the polygon. And the path between any two points have to be done within the shape and it shouldn&#39;t cross the shape.</p>

<p>How to use it?<br>
1. Draw convex hull for the points of each class<br>
2. Draw the shortest line between those hulls<br>
3. Bisect that line to get the \(\pi\) (margin maximising hyperplane)</p>

<p><img src="./1%20Support%20Vector%20Machines%20(SVM)/Screen%20Shot%202021-07-08%20at%205.42.57%20PM.png" alt=""></p>

<h2 id="toc_4">Mathematical derivation</h2>

<h3 id="toc_5">hard margin svm</h3>

<p><img src="./1%20Support%20Vector%20Machines%20(SVM)/Screen%20Shot%202021-07-08%20at%205.49.47%20PM.png" alt=""></p>

<p>But note that \(w\) is <strong>not a unit vector</strong> and \(w^Tw\neq 0\).</p>

<p><img src="./1%20Support%20Vector%20Machines%20(SVM)/Screen%20Shot%202021-07-08%20at%205.53.54%20PM.png" alt=""></p>

<p>So, <strong>our problem becomes</strong></p>

<p>\((w^*,\ b^*)\ =\ \underset{w,b}{argmax}\frac{2}{||w||}\) such that \(y_i*(w^Tx_i+b)\geq1\) for all \(x_i\)s</p>

<p><strong>But what if the +ve point in -ve region and -ve point in the +ve region (or) not linearly separable (or) almost linearly separable?</strong><br>
We can&#39;t solve the above constraint to solve for \(w,b\). So above eqn is <strong>hard margin svm</strong></p>

<hr>

<h3 id="toc_6">soft margin svm</h3>

<p><img src="./1%20Support%20Vector%20Machines%20(SVM)/Screen%20Shot%202021-07-08%20at%206.04.48%20PM.png" alt=""></p>

<p>We&#39;ll introduce \(\zeta_i\) (zeta i), for all points, such that it&#39;s value is zero for points in the proper side of the margin. For others, <strong>the more the point is away from the margin (in the incorrect direction), zeta value increases by some units</strong>. It is the <strong>distance of the point from the correct hyperplane</strong> either \(\pi^+\) or \(\pi^-\)</p>

<p>\(\underset{w,b}{argmax}\frac{2}{||w||}\) = \(\underset{w,b}{argmin}\frac{||w||}{2}\)</p>

<p>\((w^*,\ b^*)\ =\ \underset{w,b}{argmin}\frac{||w||}{2}+c*\frac{1}{n}\sum_{i=1}^n\zeta_i\)<br>
such that \(y_i*(w^Tx_i+b)\geq1-\zeta_i\) for all \(\zeta_i \geq 0\)<br>
where \(\frac{1}{n}\sum_{i=1}^n\zeta_i\) is the average distance of misclassified points from the correct hyperplane and \(c\) is the <strong>hyperparameter</strong>.</p>

<p>Here \(\frac{||w||}{2}\) is the <strong>regularization term</strong> and \(\frac{1}{n}\sum_{i=1}^n\zeta_i\) is the <strong>loss term</strong></p>

<p>As \(c\) \(\uparrow\), we are giving more importance to not make mistakes which leading to <strong>overfit</strong> &amp; <strong>high variance</strong>.<br>
As \(c\) \(\downarrow\), we are giving less importance to not make mistakes which leading to <strong>underfit</strong> &amp; <strong>high bias</strong>.</p>

<p>It is the <strong>soft margin svm</strong>.</p>

<hr>

<h3 id="toc_7">Why we take values +1 and and -1 for Support vector planes</h3>

<p>Since we are saying that \(||w||\neq1\) and it could be of <strong>any length</strong>.</p>

<p><img src="./1%20Support%20Vector%20Machines%20(SVM)/Screen%20Shot%202021-07-09%20at%2010.07.20%20AM.png" alt=""></p>

<p><img src="./1%20Support%20Vector%20Machines%20(SVM)/Screen%20Shot%202021-07-09%20at%209.50.50%20AM.png" alt=""><br>
We can take any <strong>k</strong>, as we have the optimization problem only for \(w\) as \(\frac{2}{||w||}\). So we simply take the value as \(k=1\).</p>

<h2 id="toc_8">Loss function (Hinge Loss) based interpretation</h2>

<p><img src="./1%20Support%20Vector%20Machines%20(SVM)/Screen%20Shot%202021-07-10%20at%204.05.31%20PM.png" alt=""></p>

<p>Hinge loss is <strong>not differentiable at 0</strong>. But we can handle it.<br>
Hinge loss = \(0\) if \(z_i \geq 1\)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;=\(\ (1-z_i)\) if \(z_i&lt;1\)<br>
(OR)<br>
Hinge loss = \(max(0, 1-z_i)\)</p>

<p><img src="./1%20Support%20Vector%20Machines%20(SVM)/Screen%20Shot%202021-07-10%20at%204.21.51%20PM.png" alt=""></p>

<p>Consider 2 points,<br>
1. \(x_i\) on the correct (positive) side of the \(\pi^+\), so \(\zeta_i=0\)<br>
2. \(x_j\) on the wrong side of the \(\pi\) (i.e) in the region of \(\pi^-\), so the distance of that point from \(\pi\) is \(w^Tx_j+b\) which is a <strong>-ve</strong> value. What about the distance \(d_j\) from \(\pi^+\), it is \(1-(y_j*(w^Tx_j+b))\) (also \(1-z_j\) where \(z_j\) is \((y_j*(w^Tx_j+b))\))</p>

<p><br><br>
So \(\zeta_j\ =\ (1-z_j)\) when \(x_j\) is incorrectly classified.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\(=\ 0\) when \(x_j\) is correctly classified.<br>
It is nothing but \(\zeta_j=max(0,1-z_j)\) as seen before.</p>

<p><img src="./1%20Support%20Vector%20Machines%20(SVM)/Screen%20Shot%202021-07-10%20at%204.46.30%20PM.png" alt=""></p>

<p>Both analysis (soft svm and hinge loss) are conceptually same<br>
<img src="./1%20Support%20Vector%20Machines%20(SVM)/Screen%20Shot%202021-07-10%20at%204.49.08%20PM.png" alt=""></p>

<p>[CORRECTION] : Our constraint is \((1-(y_i(w^x_i+b))\ \leq\ \zeta_i)\) (derived from \(y_i(w^x_i+b) \geq 1-\zeta_i\))</p>

<p>This is the <strong>Primal form</strong>.</p>

<h2 id="toc_9">Dual form of SVM formulation</h2>

<p>Ref : <a href="https://cs229.stanford.edu/notes2020spring/cs229-notes3.pdf">https://cs229.stanford.edu/notes2020spring/cs229-notes3.pdf</a></p>

<p>Another form is <strong>dual form</strong><br>
\(\underset{\alpha_i}{max}\sum_{i=1}^n\alpha_i-\frac{1}{2}\sum_{i=1}^n\sum_{j=1}^n\alpha_i\alpha_j y_i y_jx_i^Tx_j\)<br>
such that \(C \geq \alpha_i \geq0\) and \(\sum_{i=1}^N\alpha_iy_i=0\)<br>
Solving <strong>primal form</strong> is equivalent to this <strong>dual form</strong> mathematically.</p>

<p>It is <strong>linear SVM</strong>.</p>

<p><u>Observations :</u><br>
1. For every \(x_i\), we have \(\alpha_i\)<br>
2. \(x_i\) occur in the form of \(x_i^Tx_j\). Refers to the <strong>cosine similarity</strong> if ||\(x_i\)||=1 and ||\(x_y\)||=1 (i.e) normalized data. We can use the <strong>similarity matrix here</strong> (by substituting the same in the formula). So \(x_i^Tx_j\) will be replaced by \(k(x_i,x_j)\)<br>
3. Usually for any query point \(x_q\), we&#39;ll find \(w^x_q+b\ \) as \(f(x_q)\) and take it&#39;s sign for the class. Here, we have \(f(x_q)\) as \(\sum_{i=1}^n\alpha_iy_ix_i^Tx_q+b\). Same as 2nd point, we can use the <strong>similarity value</strong> or the <strong>kernel value</strong> as \(\sum_{i=1}^n\alpha_iy_ik(x_i,x_q)+b\)<br>
4. \(\alpha_i&gt;0\) for SVS (support vector points) and \(\alpha_i=0\) for non-SVS (support vector points). This is because we don&#39;t care about the points in either side as we can use \(f(x_q)\) and find the sign using the SVS points as ref.</p>

<p>We did this in <strong>dual form</strong> because the \(x_i,x_j\) occur in pairs and we can make use of it the <strong>kernel trick</strong>.</p>

<h2 id="toc_10">Kernel trick (to use in dual form)</h2>

<p>If we keep it as \(x_i^Tx_j\) in the optimization problem, it is called <strong>linear SVM</strong><br>
If we keep it as \(k(x_i,x_j)\), it is called <strong>kernel SVM</strong>.</p>

<p>Linear SVM is similar to <strong>logistic regression</strong> as the results don&#39;t matter much. But Kernel trick is the most <strong>important idea in SVM</strong>.</p>

<p>We can classify the <strong>non-linear separable data</strong> with the <strong>kernel SVM</strong> (like <strong>logistic regression or linear SVM + feature engineering</strong> we do for the  where we&#39;ll transform the inputs before we apply the model. So the inputs will now be in the transformed space).</p>

<p><strong>kernel function</strong> is like <strong>similarity function</strong>.</p>

<h3 id="toc_11">Polynomial Kernel</h3>

<p>Generic, \(k(x_1,x_2)=(c+x_1^Tx_2)^d\)<br>
In quadratic form and with \(c\)=1, \(k(x_1,x_2)=(1+x_1^Tx_2)^2\)</p>

<p><img src="./1%20Support%20Vector%20Machines%20(SVM)/Screen%20Shot%202021-07-10%20at%206.00.45%20PM.png" alt=""><br>
Internally, it is doing <strong>feature transformation in implicit manner</strong>.<br>
<img src="./1%20Support%20Vector%20Machines%20(SVM)/Screen%20Shot%202021-07-10%20at%206.02.03%20PM.png" alt=""><br>
\(d&#39;&gt;d\) and data is <strong>linearly separable</strong> (by <strong>Mercers theorem</strong>).</p>

<p><u>Question is</u> <em>what kernel to apply ?</em> It is all about finding right kernel in SVM.</p>

<h3 id="toc_12">RBF-Kernel</h3>

<p>Radial Basis Function :  Very very general purpose kernel. Why? Because of it&#39;s similarity concepts brought in by the \(\sigma\) term.<br>
\(k(x_1,x_2) = exp(\frac{-||x_1-x_2||^2}{2\sigma^2}) = exp(\frac{-d_{12}^2}{2\sigma^2})\) where \(\sigma\) is a hyperparameter and \(\gamma = \frac{1}{\sigma}\)</p>

<ol>
<li><p>As \(d_{12}\) \(\uparrow\), \(k(x_1,x_2)\) \(\downarrow\). It behaves like similarity.<br>
<img src="./1%20Support%20Vector%20Machines%20(SVM)/Screen%20Shot%202021-07-10%20at%206.09.23%20PM.png" alt=""></p></li>
<li><p>Keeping the \(d\) constant and varying \(\sigma\)</p></li>
</ol>

<p>\(\sigma=1\)<br>
<img src="./1%20Support%20Vector%20Machines%20(SVM)/Screen%20Shot%202021-07-10%20at%206.10.17%20PM.png" alt=""><br>
Case 1 : If the dist(\(x_1\),\(x_2\))=0, then we have kernel value as <strong>1</strong>.<br>
Case 2 : As the dist increases, the kernel values falls to <strong>0</strong> exponentially (like gaussian PDF).</p>

<p>\(\sigma=0.1\). If d&gt;1, k=0<br>
<img src="./1%20Support%20Vector%20Machines%20(SVM)/Screen%20Shot%202021-07-10%20at%206.13.59%20PM.png" alt=""><br>
As sigma got reduced and distance is between abs value of 1, we have non-zero kernel value.</p>

<p>\(\sigma=10\). If d&gt;10, k=0<br>
<img src="./1%20Support%20Vector%20Machines%20(SVM)/Screen%20Shot%202021-07-10%20at%206.15.08%20PM.png" alt=""></p>

<p>As \(\sigma\) increases, we allow more values to be similar. Equivalent to KNN.<br>
<img src="./1%20Support%20Vector%20Machines%20(SVM)/Screen%20Shot%202021-07-10%20at%206.18.48%20PM.png" alt=""></p>

<p>Similar as \(\sigma\) \(\uparrow\), to \(k\) in KNN.<br>
<img src="./1%20Support%20Vector%20Machines%20(SVM)/Screen%20Shot%202021-07-10%20at%206.21.07%20PM.png" alt=""></p>

<p>So, RBF Kernels has 2 hyper parameters:<br>
1. \(c\) from the <strong>kernel SVM</strong><br>
2. \(\sigma\) from the <strong>RBF Kernel</strong></p>

<h3 id="toc_13">Domain Specific Kernels</h3>

<ol>
<li>Genome kernel</li>
<li>String kernel</li>
<li>Graph kernel</li>
</ol>

<p>Feature transformation is partially replaced by the appropriate kernel.</p>

<h2 id="toc_14">Train and run time complexities</h2>

<p>We can use \(SGD\) algo. But we can use <strong>sequential minimal optimization (SMO)</strong> for <strong>SVM</strong>.</p>

<p><u>Training time :</u> \(O(n^2)\) for kernel SVMs. Most optimized algo will take \(O(nd^2)\). But we&#39;ll normally, we&#39;ll have \(O(n^2)\). So when \(n\) is large, <strong>SVM is not used</strong>.<br>
<u>Runtime :</u> \(f(x_q)=\sum_{i=1}^n\alpha_iy_ix_i^Tx_q+b\) It is based on no of vector points. So, complexity is \(O(kd)\) where \(d\) is the dimensionality of the input and \(1\leq k \leq n\). If <strong>k</strong> is large, then we&#39;ll have a <strong>more runtime complexity</strong>, as we <strong>can&#39;t control the no of support vectors,k</strong>.</p>

<h2 id="toc_15">nu-SVM: control errors and support vectors</h2>

<p>Original SVM is \(C-SVM\) (where \(C\geq 0\) is a hyperparameter)</p>

<p>Alternate is \(nu-SVM\)<br>
<img src="./1%20Support%20Vector%20Machines%20(SVM)/Screen%20Shot%202021-07-10%20at%206.37.31%20PM.png" alt=""></p>

<p>where we can control % of errors with \(nu\) hyperparam.</p>

<p>if \(nu=0.01\), then errors will be \(\leq\) \(1\%\) and \(\#\ SVS\) will be \(\geq\) \(1\%\) of N points.</p>

<h3 id="toc_16">Runtime complexity</h3>

<p>In nu_SVM, as we have \(\#\ SVS\) will be \(\geq\) \(1\%\) of N points, we don&#39;t have any upper bound on number of support vectors.</p>

<h2 id="toc_17">SVM Regression (SVR)</h2>

<p><img src="./1%20Support%20Vector%20Machines%20(SVM)/Screen%20Shot%202021-07-10%20at%206.44.58%20PM.png" alt=""></p>

<p>But how come this constraint will hold good? Consider the below figure with the <strong>epsilon tube</strong> (i.e) the gutter width is \(\epsilon\) above and below the central line.</p>

<p><img src="./1%20Support%20Vector%20Machines%20(SVM)/Screen%20Shot%202021-08-24%20at%204.59.35%20PM.png" alt=""><br>
<u>Case 1 :</u> <br>
Consider the point above the central line.<br>
1. \((\hat{y_i} - y_i) \leq \epsilon \longrightarrow\) so it is good.<br>
2. \((y_i - \hat{y_i})\) is \(-ve\) (i.e \(\le 0\)) and we have \(\epsilon \geq 0\). It makes  \((y_i - \hat{y_i}) \leq \epsilon \longrightarrow\) so it is good.</p>

<p><u>Case 2 :</u> <br>
Consider the point below the central line.<br>
1. \((y_i - \hat{y_i}) \leq \epsilon \longrightarrow\) so it is good.<br>
2. \((\hat{y_i} - y_i)\) is \(-ve\) (i.e \(\le 0\)) and we have \(\epsilon \geq 0\). It makes  \((y_i - \hat{y_i}) \leq \epsilon \longrightarrow\) so it is good.</p>

<p>Both the constraints are satisfied in case 1 and 2. This is only because of the \(\epsilon\) constraint we have. Consider the error case below.</p>

<p><u>Case 3 :</u><br>
Consider a point above the gutter.<br>
1. \((y_i - \hat{y_i}) \leq \epsilon \longrightarrow\) so it is good.<br>
2. \((\hat{y_i} - y_i)\) is \(+ve\) (i.e \(\ge 0\)) and we have \(\epsilon \geq 0\). It makes  \((y_i - \hat{y_i}) \gt \epsilon \longrightarrow\) so it is <strong>bad</strong>. So wrong output.</p>

<h3 id="toc_18">Bias And Variance</h3>

<p>\(\epsilon\) - hyper parameter. And also this is the number which controls the acceptable error for the input \(y_i\) and it&#39;s corresponding output \(\hat{y_i}\)<br>
As \(\epsilon\) \(\uparrow\),  errors will increase and causes <strong>underfit</strong><br>
As \(\epsilon\) \(\downarrow\),  errors will be very low in training and causes <strong>overfit</strong></p>

<p>It can also be kernalised for non-linear data regression.<br>
<img src="./1%20Support%20Vector%20Machines%20(SVM)/Screen%20Shot%202021-07-10%20at%206.45.38%20PM.png" alt=""></p>

<h2 id="toc_19">Cases</h2>

<ol>
<li>Feature Engineering and feature transformation (by finding a right kernel)</li>
<li>Decision surface is the <strong>non linear surface</strong> for non linear data and <strong>linear surface</strong> after the kernalization.</li>
<li>Similarity/distance function can be used by kernels</li>
</ol>

<p>Challenges<br>
1. We can&#39;t find the feature importance or interpretability (directly) for kernel SVMs. But we can use <strong>forward feature selection</strong><br>
2. Outliers will have very less impact as we&#39;ll use only SVS for the kernel SVM.<br>
3. RBF with small \(\sigma\) may get affected similar to the smaller <strong>k</strong> in KNN<br>
4. Large <strong>d</strong> - SVM works good. It is because of the fact that already kernel tries to convert \(d\) to \(d&#39;\) (with more features). In our hand, if we have more <strong>d</strong>, it&#39;s good then.</p>

<p>Best Case :<br>
1. Having right kernel, it works well</p>

<p>Worst Case:<br>
1. When <strong>n</strong> is large, Training time is typically long. So people will go for <strong>logistic regression</strong> by doing <strong>feature transforms</strong>.<br>
2. If we get <strong>k</strong> SVS vectors and it is huge, computing the \(f(x_q)\) will take time for the low latency systems.</p>

<h2 id="toc_20">Platt Scaling/calibration</h2>

<p>In SVM, we&#39;ll get the class based on the +ve or -ve sign of the sign(f(x)). It doesn&#39;t give the probability.</p>

<p>From the predicted output \(\hat{y_i}\) (from \(x_q\), we will use \(\hat{y_i}=y_q=f(x_q)\)), predict the probability.</p>

<p>So, Platt derived formula like</p>

<p>\(P(y_q=1|x_q) = \frac{1}{1+exp(A\ \hat{y_i}+B)}\) (modified sigmoid)</p>

<p>Dataset, \(D_{calib}=\{\hat{y_i},y_i\}\)</p>

<p>It is based on the assumption that the <strong>calibration curve</strong> will look like the sigmoid curve. The x axis represents the average predicted probability in each bin. The y axis is the fraction of positives, i.e. the proportion of samples whose class is the positive class (in each bin).<br>
<img src="./1%20Support%20Vector%20Machines%20(SVM)/Screen%20Shot%202021-08-25%20at%203.39.57%20PM.png" alt=""></p>

<p>But it may not look like this in all the cases like below. We have a stepwise calibration curve.<br>
<img src="./1%20Support%20Vector%20Machines%20(SVM)/Screen%20Shot%202021-08-25%20at%203.30.11%20PM.png" alt=""></p>

<p>So, we&#39;ll use the <strong>isotonic calibration</strong>. It is much more closer to the emprirical value. It is the <strong>most used technique</strong>.</p>



<script type="text/javascript">
var _self="undefined"!=typeof window?window:"undefined"!=typeof WorkerGlobalScope&&self instanceof WorkerGlobalScope?self:{},Prism=function(){var e=/\blang(?:uage)?-(\w+)\b/i,t=0,n=_self.Prism={util:{encode:function(e){return e instanceof a?new a(e.type,n.util.encode(e.content),e.alias):"Array"===n.util.type(e)?e.map(n.util.encode):e.replace(/&/g,"&amp;").replace(/</g,"&lt;").replace(/\u00a0/g," ")},type:function(e){return Object.prototype.toString.call(e).match(/\[object (\w+)\]/)[1]},objId:function(e){return e.__id||Object.defineProperty(e,"__id",{value:++t}),e.__id},clone:function(e){var t=n.util.type(e);switch(t){case"Object":var a={};for(var r in e)e.hasOwnProperty(r)&&(a[r]=n.util.clone(e[r]));return a;case"Array":return e.map&&e.map(function(e){return n.util.clone(e)})}return e}},languages:{extend:function(e,t){var a=n.util.clone(n.languages[e]);for(var r in t)a[r]=t[r];return a},insertBefore:function(e,t,a,r){r=r||n.languages;var l=r[e];if(2==arguments.length){a=arguments[1];for(var i in a)a.hasOwnProperty(i)&&(l[i]=a[i]);return l}var o={};for(var s in l)if(l.hasOwnProperty(s)){if(s==t)for(var i in a)a.hasOwnProperty(i)&&(o[i]=a[i]);o[s]=l[s]}return n.languages.DFS(n.languages,function(t,n){n===r[e]&&t!=e&&(this[t]=o)}),r[e]=o},DFS:function(e,t,a,r){r=r||{};for(var l in e)e.hasOwnProperty(l)&&(t.call(e,l,e[l],a||l),"Object"!==n.util.type(e[l])||r[n.util.objId(e[l])]?"Array"!==n.util.type(e[l])||r[n.util.objId(e[l])]||(r[n.util.objId(e[l])]=!0,n.languages.DFS(e[l],t,l,r)):(r[n.util.objId(e[l])]=!0,n.languages.DFS(e[l],t,null,r)))}},plugins:{},highlightAll:function(e,t){var a={callback:t,selector:'code[class*="language-"], [class*="language-"] code, code[class*="lang-"], [class*="lang-"] code'};n.hooks.run("before-highlightall",a);for(var r,l=a.elements||document.querySelectorAll(a.selector),i=0;r=l[i++];)n.highlightElement(r,e===!0,a.callback)},highlightElement:function(t,a,r){for(var l,i,o=t;o&&!e.test(o.className);)o=o.parentNode;o&&(l=(o.className.match(e)||[,""])[1],i=n.languages[l]),t.className=t.className.replace(e,"").replace(/\s+/g," ")+" language-"+l,o=t.parentNode,/pre/i.test(o.nodeName)&&(o.className=o.className.replace(e,"").replace(/\s+/g," ")+" language-"+l);var s=t.textContent,u={element:t,language:l,grammar:i,code:s};if(!s||!i)return n.hooks.run("complete",u),void 0;if(n.hooks.run("before-highlight",u),a&&_self.Worker){var c=new Worker(n.filename);c.onmessage=function(e){u.highlightedCode=e.data,n.hooks.run("before-insert",u),u.element.innerHTML=u.highlightedCode,r&&r.call(u.element),n.hooks.run("after-highlight",u),n.hooks.run("complete",u)},c.postMessage(JSON.stringify({language:u.language,code:u.code,immediateClose:!0}))}else u.highlightedCode=n.highlight(u.code,u.grammar,u.language),n.hooks.run("before-insert",u),u.element.innerHTML=u.highlightedCode,r&&r.call(t),n.hooks.run("after-highlight",u),n.hooks.run("complete",u)},highlight:function(e,t,r){var l=n.tokenize(e,t);return a.stringify(n.util.encode(l),r)},tokenize:function(e,t){var a=n.Token,r=[e],l=t.rest;if(l){for(var i in l)t[i]=l[i];delete t.rest}e:for(var i in t)if(t.hasOwnProperty(i)&&t[i]){var o=t[i];o="Array"===n.util.type(o)?o:[o];for(var s=0;s<o.length;++s){var u=o[s],c=u.inside,g=!!u.lookbehind,h=!!u.greedy,f=0,d=u.alias;u=u.pattern||u;for(var p=0;p<r.length;p++){var m=r[p];if(r.length>e.length)break e;if(!(m instanceof a)){u.lastIndex=0;var y=u.exec(m),v=1;if(!y&&h&&p!=r.length-1){var b=r[p+1].matchedStr||r[p+1],k=m+b;if(p<r.length-2&&(k+=r[p+2].matchedStr||r[p+2]),u.lastIndex=0,y=u.exec(k),!y)continue;var w=y.index+(g?y[1].length:0);if(w>=m.length)continue;var _=y.index+y[0].length,P=m.length+b.length;if(v=3,P>=_){if(r[p+1].greedy)continue;v=2,k=k.slice(0,P)}m=k}if(y){g&&(f=y[1].length);var w=y.index+f,y=y[0].slice(f),_=w+y.length,S=m.slice(0,w),O=m.slice(_),j=[p,v];S&&j.push(S);var A=new a(i,c?n.tokenize(y,c):y,d,y,h);j.push(A),O&&j.push(O),Array.prototype.splice.apply(r,j)}}}}}return r},hooks:{all:{},add:function(e,t){var a=n.hooks.all;a[e]=a[e]||[],a[e].push(t)},run:function(e,t){var a=n.hooks.all[e];if(a&&a.length)for(var r,l=0;r=a[l++];)r(t)}}},a=n.Token=function(e,t,n,a,r){this.type=e,this.content=t,this.alias=n,this.matchedStr=a||null,this.greedy=!!r};if(a.stringify=function(e,t,r){if("string"==typeof e)return e;if("Array"===n.util.type(e))return e.map(function(n){return a.stringify(n,t,e)}).join("");var l={type:e.type,content:a.stringify(e.content,t,r),tag:"span",classes:["token",e.type],attributes:{},language:t,parent:r};if("comment"==l.type&&(l.attributes.spellcheck="true"),e.alias){var i="Array"===n.util.type(e.alias)?e.alias:[e.alias];Array.prototype.push.apply(l.classes,i)}n.hooks.run("wrap",l);var o="";for(var s in l.attributes)o+=(o?" ":"")+s+'="'+(l.attributes[s]||"")+'"';return"<"+l.tag+' class="'+l.classes.join(" ")+'" '+o+">"+l.content+"</"+l.tag+">"},!_self.document)return _self.addEventListener?(_self.addEventListener("message",function(e){var t=JSON.parse(e.data),a=t.language,r=t.code,l=t.immediateClose;_self.postMessage(n.highlight(r,n.languages[a],a)),l&&_self.close()},!1),_self.Prism):_self.Prism;var r=document.currentScript||[].slice.call(document.getElementsByTagName("script")).pop();return r&&(n.filename=r.src,document.addEventListener&&!r.hasAttribute("data-manual")&&document.addEventListener("DOMContentLoaded",n.highlightAll)),_self.Prism}();"undefined"!=typeof module&&module.exports&&(module.exports=Prism),"undefined"!=typeof global&&(global.Prism=Prism);
</script>

<script type="text/javascript">
!function(){"undefined"!=typeof self&&self.Prism&&self.document&&Prism.hooks.add("complete",function(e){if(e.code){var t=e.element.parentNode,s=/\s*\bline-numbers\b\s*/;if(t&&/pre/i.test(t.nodeName)&&(s.test(t.className)||s.test(e.element.className))&&!e.element.querySelector(".line-numbers-rows")){s.test(e.element.className)&&(e.element.className=e.element.className.replace(s,"")),s.test(t.className)||(t.className+=" line-numbers");var n,a=e.code.match(/\n(?!$)/g),l=a?a.length+1:1,m=new Array(l+1);m=m.join("<span></span>"),n=document.createElement("span"),n.className="line-numbers-rows",n.innerHTML=m,t.hasAttribute("data-start")&&(t.style.counterReset="linenumber "+(parseInt(t.getAttribute("data-start"),10)-1)),e.element.appendChild(n)}}})}();
</script>

<script type="text/x-mathjax-config">
(function () {

MathJax.Hub.Config({
	'showProcessingMessages': false,
	'messageStyle': 'none'
});

if (typeof MathJaxListener !== 'undefined') {
	MathJax.Hub.Register.StartupHook('End', function () {
		MathJaxListener.invokeCallbackForKey_('End');
	});
}

})();
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


</body>

</html>
