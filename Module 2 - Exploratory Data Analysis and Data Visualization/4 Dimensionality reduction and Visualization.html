<!DOCTYPE html>
<html>

<head>

<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<title>4 Dimensionality reduction and Visualization</title>


<style type="text/css">
body {
  font-family: Helvetica, arial, sans-serif;
  font-size: 14px;
  line-height: 1.6;
  padding-top: 10px;
  padding-bottom: 10px;
  background-color: white;
  padding: 30px; }

body > *:first-child {
  margin-top: 0 !important; }
body > *:last-child {
  margin-bottom: 0 !important; }

a {
  color: #4183C4; }
a.absent {
  color: #cc0000; }
a.anchor {
  display: block;
  padding-left: 30px;
  margin-left: -30px;
  cursor: pointer;
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0; }

h1, h2, h3, h4, h5, h6 {
  margin: 20px 0 10px;
  padding: 0;
  font-weight: bold;
  -webkit-font-smoothing: antialiased;
  cursor: text;
  position: relative; }

h1:hover a.anchor, h2:hover a.anchor, h3:hover a.anchor, h4:hover a.anchor, h5:hover a.anchor, h6:hover a.anchor {
  background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA09pVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMy1jMDExIDY2LjE0NTY2MSwgMjAxMi8wMi8wNi0xNDo1NjoyNyAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvIiB4bWxuczp4bXBNTT0iaHR0cDovL25zLmFkb2JlLmNvbS94YXAvMS4wL21tLyIgeG1sbnM6c3RSZWY9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9zVHlwZS9SZXNvdXJjZVJlZiMiIHhtcDpDcmVhdG9yVG9vbD0iQWRvYmUgUGhvdG9zaG9wIENTNiAoMTMuMCAyMDEyMDMwNS5tLjQxNSAyMDEyLzAzLzA1OjIxOjAwOjAwKSAgKE1hY2ludG9zaCkiIHhtcE1NOkluc3RhbmNlSUQ9InhtcC5paWQ6OUM2NjlDQjI4ODBGMTFFMTg1ODlEODNERDJBRjUwQTQiIHhtcE1NOkRvY3VtZW50SUQ9InhtcC5kaWQ6OUM2NjlDQjM4ODBGMTFFMTg1ODlEODNERDJBRjUwQTQiPiA8eG1wTU06RGVyaXZlZEZyb20gc3RSZWY6aW5zdGFuY2VJRD0ieG1wLmlpZDo5QzY2OUNCMDg4MEYxMUUxODU4OUQ4M0REMkFGNTBBNCIgc3RSZWY6ZG9jdW1lbnRJRD0ieG1wLmRpZDo5QzY2OUNCMTg4MEYxMUUxODU4OUQ4M0REMkFGNTBBNCIvPiA8L3JkZjpEZXNjcmlwdGlvbj4gPC9yZGY6UkRGPiA8L3g6eG1wbWV0YT4gPD94cGFja2V0IGVuZD0iciI/PsQhXeAAAABfSURBVHjaYvz//z8DJYCRUgMYQAbAMBQIAvEqkBQWXI6sHqwHiwG70TTBxGaiWwjCTGgOUgJiF1J8wMRAIUA34B4Q76HUBelAfJYSA0CuMIEaRP8wGIkGMA54bgQIMACAmkXJi0hKJQAAAABJRU5ErkJggg==) no-repeat 10px center;
  text-decoration: none; }

h1 tt, h1 code {
  font-size: inherit; }

h2 tt, h2 code {
  font-size: inherit; }

h3 tt, h3 code {
  font-size: inherit; }

h4 tt, h4 code {
  font-size: inherit; }

h5 tt, h5 code {
  font-size: inherit; }

h6 tt, h6 code {
  font-size: inherit; }

h1 {
  font-size: 28px;
  color: black; }

h2 {
  font-size: 24px;
  border-bottom: 1px solid #cccccc;
  color: black; }

h3 {
  font-size: 18px; }

h4 {
  font-size: 16px; }

h5 {
  font-size: 14px; }

h6 {
  color: #777777;
  font-size: 14px; }

p, blockquote, ul, ol, dl, li, table, pre {
  margin: 15px 0; }

hr {
  background: transparent url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAYAAAAECAYAAACtBE5DAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAAyJpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvIiB4bWxuczp4bXBNTT0iaHR0cDovL25zLmFkb2JlLmNvbS94YXAvMS4wL21tLyIgeG1sbnM6c3RSZWY9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9zVHlwZS9SZXNvdXJjZVJlZiMiIHhtcDpDcmVhdG9yVG9vbD0iQWRvYmUgUGhvdG9zaG9wIENTNSBNYWNpbnRvc2giIHhtcE1NOkluc3RhbmNlSUQ9InhtcC5paWQ6OENDRjNBN0E2NTZBMTFFMEI3QjRBODM4NzJDMjlGNDgiIHhtcE1NOkRvY3VtZW50SUQ9InhtcC5kaWQ6OENDRjNBN0I2NTZBMTFFMEI3QjRBODM4NzJDMjlGNDgiPiA8eG1wTU06RGVyaXZlZEZyb20gc3RSZWY6aW5zdGFuY2VJRD0ieG1wLmlpZDo4Q0NGM0E3ODY1NkExMUUwQjdCNEE4Mzg3MkMyOUY0OCIgc3RSZWY6ZG9jdW1lbnRJRD0ieG1wLmRpZDo4Q0NGM0E3OTY1NkExMUUwQjdCNEE4Mzg3MkMyOUY0OCIvPiA8L3JkZjpEZXNjcmlwdGlvbj4gPC9yZGY6UkRGPiA8L3g6eG1wbWV0YT4gPD94cGFja2V0IGVuZD0iciI/PqqezsUAAAAfSURBVHjaYmRABcYwBiM2QSA4y4hNEKYDQxAEAAIMAHNGAzhkPOlYAAAAAElFTkSuQmCC) repeat-x 0 0;
  border: 0 none;
  color: #cccccc;
  height: 4px;
  padding: 0;
}

body > h2:first-child {
  margin-top: 0;
  padding-top: 0; }
body > h1:first-child {
  margin-top: 0;
  padding-top: 0; }
  body > h1:first-child + h2 {
    margin-top: 0;
    padding-top: 0; }
body > h3:first-child, body > h4:first-child, body > h5:first-child, body > h6:first-child {
  margin-top: 0;
  padding-top: 0; }

a:first-child h1, a:first-child h2, a:first-child h3, a:first-child h4, a:first-child h5, a:first-child h6 {
  margin-top: 0;
  padding-top: 0; }

h1 p, h2 p, h3 p, h4 p, h5 p, h6 p {
  margin-top: 0; }

li p.first {
  display: inline-block; }
li {
  margin: 0; }
ul, ol {
  padding-left: 30px; }

ul :first-child, ol :first-child {
  margin-top: 0; }

dl {
  padding: 0; }
  dl dt {
    font-size: 14px;
    font-weight: bold;
    font-style: italic;
    padding: 0;
    margin: 15px 0 5px; }
    dl dt:first-child {
      padding: 0; }
    dl dt > :first-child {
      margin-top: 0; }
    dl dt > :last-child {
      margin-bottom: 0; }
  dl dd {
    margin: 0 0 15px;
    padding: 0 15px; }
    dl dd > :first-child {
      margin-top: 0; }
    dl dd > :last-child {
      margin-bottom: 0; }

blockquote {
  border-left: 4px solid #dddddd;
  padding: 0 15px;
  color: #777777; }
  blockquote > :first-child {
    margin-top: 0; }
  blockquote > :last-child {
    margin-bottom: 0; }

table {
  padding: 0;border-collapse: collapse; }
  table tr {
    border-top: 1px solid #cccccc;
    background-color: white;
    margin: 0;
    padding: 0; }
    table tr:nth-child(2n) {
      background-color: #f8f8f8; }
    table tr th {
      font-weight: bold;
      border: 1px solid #cccccc;
      margin: 0;
      padding: 6px 13px; }
    table tr td {
      border: 1px solid #cccccc;
      margin: 0;
      padding: 6px 13px; }
    table tr th :first-child, table tr td :first-child {
      margin-top: 0; }
    table tr th :last-child, table tr td :last-child {
      margin-bottom: 0; }

img {
  max-width: 100%; }

span.frame {
  display: block;
  overflow: hidden; }
  span.frame > span {
    border: 1px solid #dddddd;
    display: block;
    float: left;
    overflow: hidden;
    margin: 13px 0 0;
    padding: 7px;
    width: auto; }
  span.frame span img {
    display: block;
    float: left; }
  span.frame span span {
    clear: both;
    color: #333333;
    display: block;
    padding: 5px 0 0; }
span.align-center {
  display: block;
  overflow: hidden;
  clear: both; }
  span.align-center > span {
    display: block;
    overflow: hidden;
    margin: 13px auto 0;
    text-align: center; }
  span.align-center span img {
    margin: 0 auto;
    text-align: center; }
span.align-right {
  display: block;
  overflow: hidden;
  clear: both; }
  span.align-right > span {
    display: block;
    overflow: hidden;
    margin: 13px 0 0;
    text-align: right; }
  span.align-right span img {
    margin: 0;
    text-align: right; }
span.float-left {
  display: block;
  margin-right: 13px;
  overflow: hidden;
  float: left; }
  span.float-left span {
    margin: 13px 0 0; }
span.float-right {
  display: block;
  margin-left: 13px;
  overflow: hidden;
  float: right; }
  span.float-right > span {
    display: block;
    overflow: hidden;
    margin: 13px auto 0;
    text-align: right; }

code, tt {
  margin: 0 2px;
  padding: 0 5px;
  white-space: nowrap;
  border: 1px solid #eaeaea;
  background-color: #f8f8f8;
  border-radius: 3px; }

pre code {
  margin: 0;
  padding: 0;
  white-space: pre;
  border: none;
  background: transparent; }

.highlight pre {
  background-color: #f8f8f8;
  border: 1px solid #cccccc;
  font-size: 13px;
  line-height: 19px;
  overflow: auto;
  padding: 6px 10px;
  border-radius: 3px; }

pre {
  background-color: #f8f8f8;
  border: 1px solid #cccccc;
  font-size: 13px;
  line-height: 19px;
  overflow: auto;
  padding: 6px 10px;
  border-radius: 3px; }
  pre code, pre tt {
    background-color: transparent;
    border: none; }

sup {
    font-size: 0.83em;
    vertical-align: super;
    line-height: 0;
}

kbd {
  display: inline-block;
  padding: 3px 5px;
  font-size: 11px;
  line-height: 10px;
  color: #555;
  vertical-align: middle;
  background-color: #fcfcfc;
  border: solid 1px #ccc;
  border-bottom-color: #bbb;
  border-radius: 3px;
  box-shadow: inset 0 -1px 0 #bbb
}

* {
	-webkit-print-color-adjust: exact;
}
@media screen and (min-width: 914px) {
    body {
        width: 854px;
        margin:0 auto;
    }
}
@media print {
	table, pre {
		page-break-inside: avoid;
	}
	pre {
		word-wrap: break-word;
	}
  body {
    padding: 2cm; 
  }
}
</style>

<style type="text/css">
/**
 * prism.js default theme for JavaScript, CSS and HTML
 * Based on dabblet (http://dabblet.com)
 * @author Lea Verou
 */

code[class*="language-"],
pre[class*="language-"] {
	color: black;
	background: none;
	text-shadow: 0 1px white;
	font-family: Consolas, Monaco, 'Andale Mono', 'Ubuntu Mono', monospace;
	text-align: left;
	white-space: pre;
	word-spacing: normal;
	word-break: normal;
	word-wrap: normal;
	line-height: 1.5;

	-moz-tab-size: 4;
	-o-tab-size: 4;
	tab-size: 4;

	-webkit-hyphens: none;
	-moz-hyphens: none;
	-ms-hyphens: none;
	hyphens: none;
}

pre[class*="language-"]::-moz-selection, pre[class*="language-"] ::-moz-selection,
code[class*="language-"]::-moz-selection, code[class*="language-"] ::-moz-selection {
	text-shadow: none;
	background: #b3d4fc;
}

pre[class*="language-"]::selection, pre[class*="language-"] ::selection,
code[class*="language-"]::selection, code[class*="language-"] ::selection {
	text-shadow: none;
	background: #b3d4fc;
}

@media print {
	code[class*="language-"],
	pre[class*="language-"] {
		text-shadow: none;
	}
}

/* Code blocks */
pre[class*="language-"] {
	padding: 1em;
	margin: .5em 0;
	overflow: auto;
}

:not(pre) > code[class*="language-"],
pre[class*="language-"] {
	background: #f5f2f0;
}

/* Inline code */
:not(pre) > code[class*="language-"] {
	padding: .1em;
	border-radius: .3em;
	white-space: normal;
}

.token.comment,
.token.prolog,
.token.doctype,
.token.cdata {
	color: slategray;
}

.token.punctuation {
	color: #999;
}

.namespace {
	opacity: .7;
}

.token.property,
.token.tag,
.token.boolean,
.token.number,
.token.constant,
.token.symbol,
.token.deleted {
	color: #905;
}

.token.selector,
.token.attr-name,
.token.string,
.token.char,
.token.builtin,
.token.inserted {
	color: #690;
}

.token.operator,
.token.entity,
.token.url,
.language-css .token.string,
.style .token.string {
	color: #a67f59;
	background: hsla(0, 0%, 100%, .5);
}

.token.atrule,
.token.attr-value,
.token.keyword {
	color: #07a;
}

.token.function {
	color: #DD4A68;
}

.token.regex,
.token.important,
.token.variable {
	color: #e90;
}

.token.important,
.token.bold {
	font-weight: bold;
}
.token.italic {
	font-style: italic;
}

.token.entity {
	cursor: help;
}
</style>

<style type="text/css">
pre.line-numbers {
	position: relative;
	padding-left: 3.8em;
	counter-reset: linenumber;
}

pre.line-numbers > code {
	position: relative;
}

.line-numbers .line-numbers-rows {
	position: absolute;
	pointer-events: none;
	top: 0;
	font-size: 100%;
	left: -3.8em;
	width: 3em; /* works for line-numbers below 1000 lines */
	letter-spacing: -1px;
	border-right: 1px solid #999;

	-webkit-user-select: none;
	-moz-user-select: none;
	-ms-user-select: none;
	user-select: none;

}

	.line-numbers-rows > span {
		pointer-events: none;
		display: block;
		counter-increment: linenumber;
	}

		.line-numbers-rows > span:before {
			content: counter(linenumber);
			color: #999;
			display: block;
			padding-right: 0.8em;
			text-align: right;
		}
</style>


</head>

<body>

<h1 id="toc_0">Dimensionality reduction and Visualization</h1>

<p>We can visualize using 2D and 3D plots. For 4D, 5D and 6D, we can leverage pair plot. As <strong>n</strong> increases, we can&#39;t use pair plot. So, we&#39;ll reduce the dimensionality of the data using PCA, t-SNE and visualize it.</p>

<p><u><strong>Column Vector:</strong></u><br>
All the \(i^{th}\) data&#39;s features will be represented as a column vector of size d x 1<br>
[\(x_{i1}, x_{i2}, ... , x_{id}\)]. It is &#39;d&#39; dimensional column vector. By default, we&#39;ll see as a column vector.</p>

<p>\(x_i\ \epsilon\ R^d\) , we&#39;ll assume that \(x_i\) is a column vector.</p>

<p><u><strong>Represent a dataset (D)</strong></u></p>

<p>Collection of data points. \(D\ =\ \{{x_i, y_i}\}_{i=1}^n\)<br>
where \(x_i\ \epsilon\ R^d\) and \(y_i\ \epsilon\ \{setosa, virginica, versicolor\}\)<br>
It is <strong>d=4</strong> dimensional, with features as [sepal length, sepal width, petal length, petal width].</p>

<p><u><strong>Dataset as data-matrix</strong></u><br>
Represented as \(n\)x\(d\) matrix and \(Y\) (\(n\)x\(1\)) column vector with output of each datapoint. Each row corresponds to one data point and each column corresponds to one feature&#39;s value. So each row is \(x_i^T\) (where \(x_i\) is a column vector). It is a much followed in industry.</p>

<p>Another way is just reverse. It is \(d\)x\(n\) matrix. It is followed in much research papers.</p>

<h2 id="toc_1">Data Preprocessing: Feature Normalisation</h2>

<p>X is \(n\)x\(d\) matrix<br>
Y is \(n\)x\(1\) column vector</p>

<p><img src="./4%20Dimensionality%20reduction%20and%20Visualization/Screen%20Shot%202021-05-30%20at%203.59.45%20PM.png" alt=""></p>

<p>Data modeling algorithms can work best after <strong>Column Normalization</strong>.</p>

<p><strong><u>Column Normalization:</u></strong></p>

<p><u>Aim:</u> to get rid of scale of any feature.</p>

<p>Let [\(a_1\), \(a_2\), \(a_3\) ... \(a_i\) ... \(a_n\)] be the value of the feature \(f_j\) for all the \(n\) data points. \(a_{max}\) be max of values annd \(a_{min}\) be min of values.</p>

<p>\(a&#39;_i\) = \(\frac{a_i-a_{min}}{a_{max}-a_{min}}\ \epsilon\ [0,1] \)</p>

<p>New array&#39;s values be [\(a&#39;_1\), \(a&#39;_2\), \(a&#39;_3\) ... \(a&#39;_n\)] =&gt; all numbers will be between \(0\) and \(1\).</p>

<p>Why? Data without normalization will add more weight to it&#39;s priority. This allow features to be in any scale (before normalization). We are bringing all values inside a unit square in the geometric scale.</p>

<p><strong><u>Mean Vector:</u></strong></p>

<p>Just like scalars, we&#39;ll have scale vectors across <strong>d</strong> features. We&#39;ll take mean of all features and put in the vector.</p>

<p><strong><u>Column Standardization:</u></strong></p>

<p><u>Aim :</u> To convert the values to follow Normal Distribution</p>

<p>More often used rather than column normalization. Let <a href="of%20any%20distribution">\(a_1\), \(a_2\), \(a_3\) ... \(a_i\) ... \(a_n\)</a> be the value of the feature \(f_j\) for all the \(n\) data points. Convert those to <strong>normal distribution</strong>.</p>

<p>\(a&#39;_i\) = \(\frac{a_i-\bar{a}}{s}\) where \(\bar{a}\) is the sample mean and \(s\) is the sample standard deviation.</p>

<p>\(mean(a&#39;_i)_{i=1}^n=0\) and \(std-dev(a&#39;_i)_{i=1}^n=1\)</p>

<p>Geometrically,<br>
<img src="./4%20Dimensionality%20reduction%20and%20Visualization/Screen%20Shot%202021-05-30%20at%204.38.28%20PM.png" alt=""></p>

<ol>
<li>Move points to (0,0) - <strong>Mean centering</strong></li>
<li>Squeeze/Expand to match the standard deviation as 1 - <strong>Scaling</strong></li>
</ol>

<p><strong><u>Co-variance of a Data Matrix:</u></strong></p>

<p>Input matrix with size \(n\)x\(d\), will have a covariance matrix \(S\) of size \(d\)x\(d\).</p>

<p>\(S_{ij}\) = \(cov(f_i,f_j)\)<br>
\(cov(f_i,f_i)\) = \(var(f_i)\) as we have seen in prob &amp; stat<br>
\(cov(f_i,f_j)\) = \(cov(f_j,f_i)\)</p>

<p>So the matrix \(S\) will have diagonal matrix values as variances of the corresponding features and it is symmetrix matrix.</p>

<p>Let X be column standardized. So, mean(\(f_i\))=0 and std(\(f_i\))=1. This will affect the covariance formula as dot product of those 2 features.<br>
\(cov(f_1,f_2)=\frac{\sum_{i=1}^n{(x_{1i}-\mu_1)(x_{2i}-\mu_2)}}{n-1}\) = \(\frac{\sum_{i=1}^n{(x_{1i})(x_{2i})}}{n-1}\) = \(\frac{f_1^T.f_2}{n-1}\)</p>

<p>After columns standardization,<br>
\(S\) = \(\frac{1}{n-1} (X^T)(X)\)</p>

<p><strong>MNIST Database:</strong><br>
Dataset of hand written numerical digits with 28x28 pixels each. 60k datapoints and 10k test data points. Objective is to classify the written character,</p>

<p>X - 2D array of grayscale value of the numbers.<br>
<img src="./4%20Dimensionality%20reduction%20and%20Visualization/Screen%20Shot%202021-05-30%20at%206.42.25%20PM.png" alt=""></p>

<p><img src="./4%20Dimensionality%20reduction%20and%20Visualization/Screen%20Shot%202021-05-30%20at%206.43.14%20PM.png" alt=""><br>
We&#39;ll do row-flattening to get our each data point as a single column vector. This vector will have \(784\)x\(1\) dimension. We&#39;ll put the transpose of it in the input vector.</p>

<p>So, the dimension of X is \(60k\)x\(784\) This is 784 dimensional dataset. How do we visualize it? using t-sne, by converting 784 dimension to 2 dimension.</p>

<h2 id="toc_2">PCA (Principal Component Analysis)</h2>

<p>It is very basic and old.</p>

<p>For dimensionality reduction from \(d \rightarrow d&#39;\) such that \(d&#39;\)&lt; \(d\) . And also for data visualization by reducing \(d\) to \(2\).</p>

<p>Example convering \(2d\) to\(1d\). The concept will hold good for higher dimensions.<br>
X is \(n\)x\(2\). The features are black shades of hair and height of hair. Spread (variance) of black shades will be very low and we can ignore it by choosing only the height of the hair. So \(X\) will become \(X&#39;\) only with height of hair (since it has wide variance). Here I am keeping the features with more spread (variance).</p>

<p>Assume an \(X\) with column standardization done for the 2 features. Here variance is 1 for both features.</p>

<p><img src="./4%20Dimensionality%20reduction%20and%20Visualization/Screen%20Shot%202021-05-30%20at%208.17.38%20PM.png" alt=""></p>

<p>Find \(f&#39;_1\) where the variance is more and \(f&#39;_2\) perpendicular to the other. We want to find a direction \(f&#39;_1\) such that the variance of \(x_i\)&#39;s projected onto \(f&#39;_1\) is maximal.</p>

<p>Let \(f&#39;_1\) be \(u_1\). We just need to find the direction of that. \(u_1\) is unit vector.</p>

<p><img src="./4%20Dimensionality%20reduction%20and%20Visualization/Screen%20Shot%202021-05-30%20at%208.30.31%20PM.png" alt=""></p>

<p>\(x&#39;_i\) is the project of \(x_i\) onto \(\vec{u}\).<br>
\(x&#39;_i\) = \(proj_{u_1}x_i\) = \(\frac{u_1\ .\ x_i}{||u_1||}\) = \(u_1^T x_i\)</p>

<p>we can find the <strong>mean vector</strong> in the direction of \(\vec{u_1}\) as<br>
\(\bar{x&#39;}\) = \(u_1^T\ \bar{x}\) where \(\bar{x}\) is the mean vector.</p>

<p>Now task in hand is find \(u_1\) such that variance(\(proj_{u_1}x_i\)) is maximum<br>
<strong>variance</strong>(\(proj_{u_1}x_i\)) = \(\frac{1}{n} \sum_{i=1}^n (u_1^T x_i - u_1^T \bar{x})^2 \)</p>

<p>since the data is standard normalized, mean is 0, which means \(u_1^T \bar{x}\) is 0.<br>
<strong>variance</strong>(\(proj_{u_1}x_i\)) = \(\frac{1}{n} \sum_{i=1}^n (u_1^T x_i )^2 \)<br>
Now maximize that (i.e) \(max_{u_1}\ \frac{1}{n} \sum_{i=1}^n (u_1^T x_i )^2 \) such that \(u^Tu=1\) and \(||u||=1\)</p>

<p>This is <strong>Variance maximization PCA</strong> and an optimization problem.</p>

<h3 id="toc_3">Alternative formulation of PCA: Distance minimization</h3>

<p><img src="./4%20Dimensionality%20reduction%20and%20Visualization/Screen%20Shot%202021-05-30%20at%208.50.08%20PM.png" alt=""></p>

<p><img src="./4%20Dimensionality%20reduction%20and%20Visualization/Screen%20Shot%202021-05-30%20at%208.52.01%20PM.png" alt=""></p>

<p>\(min_{u_1} \sum_{i=1}^n (x_i^Tx_i - (u_1^Tx_i)^2) \) such that \(u^Tu=1\) and \(||u||=1\)</p>

<p>This is <strong>Distance minimization PCA</strong> and an optimization problem.</p>

<h2 id="toc_4">Eigen values and Eigen vectors (PCA): Dimensionality reduction</h2>

<p>Solution to our optimization problems.</p>

<p>X has dimension \(n\)x\(d\) and they are column standardized. \(S\) is covariance matrix of dimension \(d\)x\(d\).</p>

<p>Eigen values (\(\lambda_1\), \(\lambda_2\) ... \(\lambda_d\))<br>
Eigen vectors (\(V_1\), \(V_2\) ... \(V_d\))</p>

<p>If I compute eigen values of \(S\), we&#39;ll get \(d\) eigen values and corresponding eigen vector \(V_i\) for each \(\lambda_i\).</p>

<p>Let&#39;s assume that \(\lambda_1\) \(\geq\) \(\lambda_2\) \(\geq\) ... \(\geq\) \(\lambda_d\).</p>

<p><u>Definition of eigen value and vector :</u> If I can find a \(\lambda\) such that \(\lambda_iV_i\) = \(SV_i\), then \(\lambda_i\) is eigen value and \(V_i\) is eigen vector. The eigenvalues in PCA tell you how <strong>much variance</strong> can be explained by its associated eigenvector.</p>

<p>Property of eigen vector is that they are all <strong>perpendicular</strong> to each other (i.e.) \(V_i \perp V_j\). So \(V_i^TV_j=0\)</p>

<p>Now, \(u_1\)=\(V_1\) = eigen vector of \(S\) corresponding to the largest eigen value \(\lambda_1\). But why? (we&#39;ll see it later)</p>

<p><img src="./4%20Dimensionality%20reduction%20and%20Visualization/Screen%20Shot%202021-05-30%20at%209.10.17%20PM.png" alt=""></p>

<p><img src="./4%20Dimensionality%20reduction%20and%20Visualization/Screen%20Shot%202021-05-30%20at%209.11.43%20PM.png" alt=""></p>

<p>Let&#39;s say \(X\ \epsilon R^{10}\), so we&#39;ll have 10 eigen values and vectors such that \(\lambda_1\) \(\geq\) \(\lambda_2\) \(\geq\) ... \(\geq\) \(\lambda_{10}\) in the order of maximum variance. This is the <strong>geometric representation</strong>.</p>

<p><img src="./4%20Dimensionality%20reduction%20and%20Visualization/Screen%20Shot%202021-05-30%20at%209.17.43%20PM.png" alt=""></p>

<p>From above image, consider 2 lambda&#39;s \(\lambda_1\) and \(\lambda_2\), if \(\frac{\lambda_1}{\lambda_1+\lambda_2}=\frac{3}{5}=0.6\) and we chose to remove \(\lambda_2\), then we will lose 40% of information. So, we can see how much of variance explained by the eigen vector.</p>

<h3 id="toc_5">PCA for Dimensionality Reduction and Visualization</h3>

<p><img src="./4%20Dimensionality%20reduction%20and%20Visualization/Screen%20Shot%202021-05-30%20at%209.24.10%20PM.png" alt=""></p>

<p><img src="./4%20Dimensionality%20reduction%20and%20Visualization/Screen%20Shot%202021-05-30%20at%209.27.58%20PM.png" alt=""></p>

<p><img src="./4%20Dimensionality%20reduction%20and%20Visualization/Screen%20Shot%202021-05-30%20at%209.29.04%20PM.png" alt=""></p>

<p>Consider the case where I want to reduce from \(d\)=100 to some \(d&#39;\) where \(d &lt; d&#39;\) where I will <strong>preserve the 99% of variance</strong>. I can do till what eigen value I&#39;ll get 0.99</p>

<p>\(\frac{\lambda_1+\lambda_2+...\lambda_{d&#39;}}{\sum_{i=1}^{100} \lambda_i}\)</p>

<p>Then we can choose \(d&#39;\) components/features</p>

<h3 id="toc_6">Visualizing MNIST</h3>

<p><a href="https://colah.github.io/posts/2014-10-Visualizing-MNIST/">https://colah.github.io/posts/2014-10-Visualizing-MNIST/</a></p>

<p><img src="./4%20Dimensionality%20reduction%20and%20Visualization/Screen%20Shot%202021-05-31%20at%204.49.50%20PM.png" alt=""></p>

<p>784 dimensions are reduced to 2 dimensions. 1&#39;s and 0&#39;s are grouped together. We can see grouping of data.</p>

<p><strong><q>we can&#39;t interpret what are those 2 dimensions</q> : we can&#39;t interpret whether these features belong to the already existing features or they are derived from a number of other existing features to form 2 new features that capture most of the Data variance.</strong></p>

<p><strong><u>Limitations of PCA:</u></strong><br>
We&#39;ll lose so much data when we project the data onto the vector direction of major spread.. Check the image with many cases.</p>

<p><img src="./4%20Dimensionality%20reduction%20and%20Visualization/Screen%20Shot%202021-05-31%20at%204.58.29%20PM.png" alt=""></p>

<p>We can also use PCA to reduce dimensions to smaller number.</p>

<h3 id="toc_7">Python (visualization)</h3>

<div><pre class="line-numbers"><code class="language-none">from sklearn import decomposition
from sklearn.preprocessing import StandardScaler

data = pd.read_csv(&#39;/content/drive/MyDrive/AAIC/Datasets/mnist_train.csv&#39;)
X = data.drop(&#39;label&#39;, axis=1)
Y = data[&#39;label&#39;]

s_data = StandardScaler().fit_transform(data)

pca=decomposition.PCA(n_components=2)
pca_data = pca.fit_transform(s_data)
final_data = np.vstack((pca_data.T, labels)).T #since the shape of labels is (N,), vstack will convert that to (1,N)df = pd.DataFrame(data=final_data, columns=[&quot;1st principal&quot;, &quot;2nd principal&quot;, &quot;Labels&quot;])
sns.set_style(&quot;whitegrid&quot;)
sns.FacetGrid(df, hue=&#39;Labels&#39;, size=8).map(plt.scatter, &#39;1st principal&#39;, &#39;2nd principal&#39;).add_legend()
plt.show()</code></pre></div>

<p><img src="./4%20Dimensionality%20reduction%20and%20Visualization/pca1.png" alt=""></p>

<h3 id="toc_8">Python (Dimensionality reduction)</h3>

<div><pre class="line-numbers"><code class="language-none">pca = decomposition.PCA()
pca_data = pca.fit_transform(s_data)

variance = pca.explained_variance_ #all the variance values

#variance
cum_variance = np.cumsum(variance/sum(variance))

fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))
axes[0].plot(lambdas, np.arange(len(lambdas)) , color=&#39;green&#39;, label=&#39;Eigen values&#39; )
axes[0].set_xlabel(&#39;784 features&#39;)
axes[1].plot(np.arange(len(var_ratio)), cum_variance , color=&#39;red&#39;, label=&#39;%variance explained&#39;)
axes[1].set_xlabel(&#39;784 features&#39;)
plt.legend()

plt.show()
</code></pre></div>

<p><img src="./4%20Dimensionality%20reduction%20and%20Visualization/pca2.png" alt=""></p>

<h2 id="toc_9">What is t-SNE (t-distributed Stochastic Neighbourhood Embedding)?</h2>

<p>One of the best dimensionality visualization of data. We also have multidimensional scaling, sammon mapping, graph based tech.</p>

<p>PCA preserves the global shape of the data, ignoring the local shapes. But t-sne preserves the local structure.</p>

<p><img src="./4%20Dimensionality%20reduction%20and%20Visualization/Screen%20Shot%202021-05-31%20at%206.56.34%20PM.png" alt=""></p>

<h3 id="toc_10">Neighborhood of a point</h3>

<p>\(x_i\) is the point and the circle represents the neighbourhood \(N(x_i)\) of that point</p>

<p>\(N(x_i)\) = {\(x_j\), such that \(x_i\) and \(x_j\) are geometrically close}</p>

<p><img src="./4%20Dimensionality%20reduction%20and%20Visualization/Screen%20Shot%202021-05-31%20at%207.01.06%20PM.png" alt=""></p>

<h3 id="toc_11">Embedding</h3>

<p>For every point in higher dimension, if I can find a point in lower dimension, it is called embedding.<br>
<img src="./4%20Dimensionality%20reduction%20and%20Visualization/Screen%20Shot%202021-05-31%20at%207.03.36%20PM.png" alt=""></p>

<h3 id="toc_12">Geometric intuition of t-SNE</h3>

<p>In d-dim, \(N(x_1)=\{x_2, x_3\}\) and \(N(x_4)=\{x_5\}\). While mapping to lower dimension, each point&#39;s neighbourhood distance should be preserved. Distance between 2 neighbourhood won&#39;t be preserved in the lower dimension.</p>

<p><img src="./4%20Dimensionality%20reduction%20and%20Visualization/Screen%20Shot%202021-05-31%20at%207.11.55%20PM.png" alt=""></p>

<p><u>Mathematical Formulation</u> is fairly advanced.</p>

<h3 id="toc_13">Crowding Problem</h3>

<p>Consider a square in 2D to be mapped to 1D.</p>

<p><img src="./4%20Dimensionality%20reduction%20and%20Visualization/Screen%20Shot%202021-05-31%20at%207.21.36%20PM.png" alt=""></p>

<p>\(N(x_1)=\{x_2,\ x_4\}\)<br>
\(N(x_3)=\{x_2,\ x_4\}\)<br>
We can&#39;t place \(x_4\) properly. Sometimes it is <strong>impossible</strong> to preserve distances in all the neighbourhoods. It is called <strong>crowding problem</strong>.</p>

<h3 id="toc_14">How to apply t-SNE and interpret its output</h3>

<p>Ref : <a href="https://distill.pub/2016/misread-tsne/">https://distill.pub/2016/misread-tsne/</a></p>

<p>Iterative method, which will stop eventually when clusters are no more moving. In each step, it&#39;ll do the embedding.</p>

<p><u><strong>Parameters :</strong></u><br>
Steps - <code>int</code> - no of iterations. More iteration more better. We can stop when the clusters are not moving.<br>
Perplexity - <code>int</code> - it is no of neighbours to be preserved for each point.<br>
Epsilon - <code>int</code> - how much one should change the cluster from one step to another. It is an optimization parameter.</p>

<p><u>Notes</u><br>
- Always run t-sne with <strong>multiple perplexity values</strong> to understand actual shape<br>
- If perplexity=no of data point, it is a <strong>mess</strong>.<br>
- Run with different steps and perplexity till the cluster&#39;s shape isn&#39;t <strong>stabilized</strong>.<br>
- t-SNE is a <strong>stochastic/probabilistic algorithm</strong> as we may find slight different cluster for the same steps and perplexity. So run many times with same step and perplexity.<br>
- <strong>Cluster size in t-SNE plot doesn&#39;t matter</strong> (i.e.) the spread of the data doesn&#39;t matter as long as they are in cluster.<br>
- <strong>Distance between clusters</strong> doesn&#39;t mean anything.<br>
- Random noise doesn’t always look random. They may try to cluster with each perplexity (especially smaller values). They may have some meaning.<br>
- For topology, you may need more than one plot (i.e) if set of points inside another set of points, we need more than one plot for confirmation.</p>

<p><u>Steps:</u><br>
1. Run the steps till the clusters/shape stabilize<br>
2. Try with multiple perplexity values \(2\leq p\leq n\)<br>
3. Re-run with same steps and perplexity to confirm the cluster separation or stability</p>

<p>There’s a reason that t-SNE has become so popular: it’s incredibly flexible, and can often find structure where other dimensionality-reduction algorithms cannot. Unfortunately, that very flexibility makes it tricky to interpret. Out of sight from the user, the algorithm makes all sorts of adjustments that tidy up its visualizations. Don’t let the hidden “magic” scare you away from the whole technique, though. The good news is that by studying how t-SNE behaves in simple cases, it’s possible to develop an intuition for what’s going on.</p>

<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<u><strong><em>It&#39;ll cluster points based on their visual similarity</em></strong></u>.</p>

<h3 id="toc_15">python</h3>

<div><pre class="line-numbers"><code class="language-none">from sklearn.manifold import TSNE

s1000 = s_data[0:1000,]
l1000 = labels[0:1000]

def do_tsne(s, l, perp=30, iter=1000):
  model = TSNE(n_components=2, perplexity=perp, random_state=0, n_iter=iter)
  tsne_data = model.fit_transform(s)
  tsne_data = np.vstack((tsne_data.T, l)).T
  tsne_df = pd.DataFrame(data=tsne_data, columns=(&quot;Dim_1&quot;, &quot;Dim_2&quot;, &quot;label&quot;))
    
  sn.FacetGrid(tsne_df, hue=&quot;label&quot;, size=6).map(plt.scatter, &#39;Dim_1&#39;, &#39;Dim_2&#39;).add_legend()
  plt.title(&#39;With perplexity = 50&#39;)
  plt.show()
  
do_tsne(s1000, l1000, 50)</code></pre></div>

<p><img src="./4%20Dimensionality%20reduction%20and%20Visualization/pca3.png" alt=""></p>

<h2 id="toc_16">Interview questions</h2>

<p>You are given a train data set having 1000 columns and 1 million rows. The data set is based on a classification problem. Your manager has asked you to reduce the dimension of this data so that model computation time can be reduced. Your machine has memory constraints. What would you do? (You are free to make practical assumptions.)</p>

<p><a href="https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/">https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/</a></p>

<p>Is rotation necessary in PCA? If yes, Why? <a href="https://google-interview-hacks.blogspot.com/2017/04/is-rotation-necessary-in-pca-if-yes-why.html">https://google-interview-hacks.blogspot.com/2017/04/is-rotation-necessary-in-pca-if-yes-why.html</a></p>

<p>You are given a data set. The data set contains many variables, some of which are highly correlated and you know about it. Your manager has asked you to run PCA. Would you remove correlated variables first? Why?<a href="https://www.linkedin.com/pulse/questions-machine-learning-statistics-can-you-answer-saraswat/">https://www.linkedin.com/pulse/questions-machine-learning-statistics-can-you-answer-saraswat/</a></p>



<script type="text/javascript">
var _self="undefined"!=typeof window?window:"undefined"!=typeof WorkerGlobalScope&&self instanceof WorkerGlobalScope?self:{},Prism=function(){var e=/\blang(?:uage)?-(\w+)\b/i,t=0,n=_self.Prism={util:{encode:function(e){return e instanceof a?new a(e.type,n.util.encode(e.content),e.alias):"Array"===n.util.type(e)?e.map(n.util.encode):e.replace(/&/g,"&amp;").replace(/</g,"&lt;").replace(/\u00a0/g," ")},type:function(e){return Object.prototype.toString.call(e).match(/\[object (\w+)\]/)[1]},objId:function(e){return e.__id||Object.defineProperty(e,"__id",{value:++t}),e.__id},clone:function(e){var t=n.util.type(e);switch(t){case"Object":var a={};for(var r in e)e.hasOwnProperty(r)&&(a[r]=n.util.clone(e[r]));return a;case"Array":return e.map&&e.map(function(e){return n.util.clone(e)})}return e}},languages:{extend:function(e,t){var a=n.util.clone(n.languages[e]);for(var r in t)a[r]=t[r];return a},insertBefore:function(e,t,a,r){r=r||n.languages;var l=r[e];if(2==arguments.length){a=arguments[1];for(var i in a)a.hasOwnProperty(i)&&(l[i]=a[i]);return l}var o={};for(var s in l)if(l.hasOwnProperty(s)){if(s==t)for(var i in a)a.hasOwnProperty(i)&&(o[i]=a[i]);o[s]=l[s]}return n.languages.DFS(n.languages,function(t,n){n===r[e]&&t!=e&&(this[t]=o)}),r[e]=o},DFS:function(e,t,a,r){r=r||{};for(var l in e)e.hasOwnProperty(l)&&(t.call(e,l,e[l],a||l),"Object"!==n.util.type(e[l])||r[n.util.objId(e[l])]?"Array"!==n.util.type(e[l])||r[n.util.objId(e[l])]||(r[n.util.objId(e[l])]=!0,n.languages.DFS(e[l],t,l,r)):(r[n.util.objId(e[l])]=!0,n.languages.DFS(e[l],t,null,r)))}},plugins:{},highlightAll:function(e,t){var a={callback:t,selector:'code[class*="language-"], [class*="language-"] code, code[class*="lang-"], [class*="lang-"] code'};n.hooks.run("before-highlightall",a);for(var r,l=a.elements||document.querySelectorAll(a.selector),i=0;r=l[i++];)n.highlightElement(r,e===!0,a.callback)},highlightElement:function(t,a,r){for(var l,i,o=t;o&&!e.test(o.className);)o=o.parentNode;o&&(l=(o.className.match(e)||[,""])[1],i=n.languages[l]),t.className=t.className.replace(e,"").replace(/\s+/g," ")+" language-"+l,o=t.parentNode,/pre/i.test(o.nodeName)&&(o.className=o.className.replace(e,"").replace(/\s+/g," ")+" language-"+l);var s=t.textContent,u={element:t,language:l,grammar:i,code:s};if(!s||!i)return n.hooks.run("complete",u),void 0;if(n.hooks.run("before-highlight",u),a&&_self.Worker){var c=new Worker(n.filename);c.onmessage=function(e){u.highlightedCode=e.data,n.hooks.run("before-insert",u),u.element.innerHTML=u.highlightedCode,r&&r.call(u.element),n.hooks.run("after-highlight",u),n.hooks.run("complete",u)},c.postMessage(JSON.stringify({language:u.language,code:u.code,immediateClose:!0}))}else u.highlightedCode=n.highlight(u.code,u.grammar,u.language),n.hooks.run("before-insert",u),u.element.innerHTML=u.highlightedCode,r&&r.call(t),n.hooks.run("after-highlight",u),n.hooks.run("complete",u)},highlight:function(e,t,r){var l=n.tokenize(e,t);return a.stringify(n.util.encode(l),r)},tokenize:function(e,t){var a=n.Token,r=[e],l=t.rest;if(l){for(var i in l)t[i]=l[i];delete t.rest}e:for(var i in t)if(t.hasOwnProperty(i)&&t[i]){var o=t[i];o="Array"===n.util.type(o)?o:[o];for(var s=0;s<o.length;++s){var u=o[s],c=u.inside,g=!!u.lookbehind,h=!!u.greedy,f=0,d=u.alias;u=u.pattern||u;for(var p=0;p<r.length;p++){var m=r[p];if(r.length>e.length)break e;if(!(m instanceof a)){u.lastIndex=0;var y=u.exec(m),v=1;if(!y&&h&&p!=r.length-1){var b=r[p+1].matchedStr||r[p+1],k=m+b;if(p<r.length-2&&(k+=r[p+2].matchedStr||r[p+2]),u.lastIndex=0,y=u.exec(k),!y)continue;var w=y.index+(g?y[1].length:0);if(w>=m.length)continue;var _=y.index+y[0].length,P=m.length+b.length;if(v=3,P>=_){if(r[p+1].greedy)continue;v=2,k=k.slice(0,P)}m=k}if(y){g&&(f=y[1].length);var w=y.index+f,y=y[0].slice(f),_=w+y.length,S=m.slice(0,w),O=m.slice(_),j=[p,v];S&&j.push(S);var A=new a(i,c?n.tokenize(y,c):y,d,y,h);j.push(A),O&&j.push(O),Array.prototype.splice.apply(r,j)}}}}}return r},hooks:{all:{},add:function(e,t){var a=n.hooks.all;a[e]=a[e]||[],a[e].push(t)},run:function(e,t){var a=n.hooks.all[e];if(a&&a.length)for(var r,l=0;r=a[l++];)r(t)}}},a=n.Token=function(e,t,n,a,r){this.type=e,this.content=t,this.alias=n,this.matchedStr=a||null,this.greedy=!!r};if(a.stringify=function(e,t,r){if("string"==typeof e)return e;if("Array"===n.util.type(e))return e.map(function(n){return a.stringify(n,t,e)}).join("");var l={type:e.type,content:a.stringify(e.content,t,r),tag:"span",classes:["token",e.type],attributes:{},language:t,parent:r};if("comment"==l.type&&(l.attributes.spellcheck="true"),e.alias){var i="Array"===n.util.type(e.alias)?e.alias:[e.alias];Array.prototype.push.apply(l.classes,i)}n.hooks.run("wrap",l);var o="";for(var s in l.attributes)o+=(o?" ":"")+s+'="'+(l.attributes[s]||"")+'"';return"<"+l.tag+' class="'+l.classes.join(" ")+'" '+o+">"+l.content+"</"+l.tag+">"},!_self.document)return _self.addEventListener?(_self.addEventListener("message",function(e){var t=JSON.parse(e.data),a=t.language,r=t.code,l=t.immediateClose;_self.postMessage(n.highlight(r,n.languages[a],a)),l&&_self.close()},!1),_self.Prism):_self.Prism;var r=document.currentScript||[].slice.call(document.getElementsByTagName("script")).pop();return r&&(n.filename=r.src,document.addEventListener&&!r.hasAttribute("data-manual")&&document.addEventListener("DOMContentLoaded",n.highlightAll)),_self.Prism}();"undefined"!=typeof module&&module.exports&&(module.exports=Prism),"undefined"!=typeof global&&(global.Prism=Prism);
</script>

<script type="text/javascript">
!function(){"undefined"!=typeof self&&self.Prism&&self.document&&Prism.hooks.add("complete",function(e){if(e.code){var t=e.element.parentNode,s=/\s*\bline-numbers\b\s*/;if(t&&/pre/i.test(t.nodeName)&&(s.test(t.className)||s.test(e.element.className))&&!e.element.querySelector(".line-numbers-rows")){s.test(e.element.className)&&(e.element.className=e.element.className.replace(s,"")),s.test(t.className)||(t.className+=" line-numbers");var n,a=e.code.match(/\n(?!$)/g),l=a?a.length+1:1,m=new Array(l+1);m=m.join("<span></span>"),n=document.createElement("span"),n.className="line-numbers-rows",n.innerHTML=m,t.hasAttribute("data-start")&&(t.style.counterReset="linenumber "+(parseInt(t.getAttribute("data-start"),10)-1)),e.element.appendChild(n)}}})}();
</script>

<script type="text/x-mathjax-config">
(function () {

MathJax.Hub.Config({
	'showProcessingMessages': false,
	'messageStyle': 'none'
});

if (typeof MathJaxListener !== 'undefined') {
	MathJax.Hub.Register.StartupHook('End', function () {
		MathJaxListener.invokeCallbackForKey_('End');
	});
}

})();
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


</body>

</html>
