#Classification Interview Questions

##Take test

[https://machinelearninginterview.com/machine-learning-interview-questions/](https://machinelearninginterview.com/machine-learning-interview-questions/)

##Revision

[What is Imbalanced and  balanced dataset.](https://www.appliedaicourse.com/lecture/11/applied-machine-learning-online-course/2958/imbalanced-vs-balanced-dataset/3/module-3-foundations-of-natural-language-processing-and-machine-learning)
[Define Multi-class classification?](https://www.appliedaicourse.com/lecture/11/applied-machine-learning-online-course/2959/multi-class-classification/3/module-3-foundations-of-natural-language-processing-and-machine-learning)
[Explain Impact of Outliers?](https://www.appliedaicourse.com/lecture/11/applied-machine-learning-online-course/2962/impact-of-outliers/3/module-3-foundations-of-natural-language-processing-and-machine-learning)
[What is Local Outlier Factor?](https://www.appliedaicourse.com/lecture/11/applied-machine-learning-online-course/2963/local-outlier-factor-simple-solution-mean-distance-to-knn/3/module-3-foundations-of-natural-language-processing-and-machine-learning)
[What is k-distance (A), N(A)](https://www.appliedaicourse.com/lecture/11/applied-machine-learning-online-course/2964/k-distance/3/module-3-foundations-of-natural-language-processing-and-machine-learning)
[Define reachability-distance(A, B)?](https://www.appliedaicourse.com/lecture/11/applied-machine-learning-online-course/2965/reachability-distanceab/3/module-3-foundations-of-natural-language-processing-and-machine-learning)
[What is Local-reachability-density(A)?](https://www.appliedaicourse.com/lecture/11/applied-machine-learning-online-course/2966/local-reachability-densitya/3/module-3-foundations-of-natural-language-processing-and-machine-learning)
[Define LOF(A)?](https://www.appliedaicourse.com/lecture/11/applied-machine-learning-online-course/2967/local-outlier-factora/3/module-3-foundations-of-natural-language-processing-and-machine-learning)
[Impact of Scale & Column standardization?](https://www.appliedaicourse.com/lecture/11/applied-machine-learning-online-course/2968/impact-of-scale-column-standardization/3/module-3-foundations-of-natural-language-processing-and-machine-learning)
[What is Interpretability?](https://www.appliedaicourse.com/lecture/11/applied-machine-learning-online-course/2969/interpretability/3/module-3-foundations-of-natural-language-processing-and-machine-learning)
[Handling categorical and numerical features?](https://www.appliedaicourse.com/lecture/11/applied-machine-learning-online-course/2971/handling-categorical-and-numerical-features/3/module-3-foundations-of-natural-language-processing-and-machine-learning)
[Handling missing values by imputation?](https://www.appliedaicourse.com/lecture/11/applied-machine-learning-online-course/2977/handling-missing-values-by-imputation/3/module-3-foundations-of-natural-language-processing-and-machine-learning)
[Bias-Variance tradeoff?](https://www.appliedaicourse.com/lecture/11/applied-machine-learning-online-course/2973/bias-variance-tradeoff/3/module-3-foundations-of-natural-language-processing-and-machine-learning)

##Interview Questions
**Methods of Bias-variance trade-off**

1. Understanding Over- and Under-Fitting :- At its root, dealing with bias and variance is really about dealing with over- and under-fitting. Bias is reduced and variance is increased in relation to model complexity. As more and more parameters are added to a model, the complexity of the model rises and variance becomes our primary concern while bias steadily falls. The following tricks are employed to fix a **high bias problem**:

	a - Train more complex model 
	b - Obtain more features 
	c - Decrease regularization etc 
	
	In most of the situation, your model is complex enough that it overfits your data. The following tricks should be employed to **deal with overfitting**. 
	
	a - Obtain more data 
	b - Decrease number of features 
	c - Increase regularization etc. 
	
2. Another way of resolving the trade-off is to use [mixture models](https://en.wikipedia.org/wiki/Mixture_model) and [ensemble learning](https://en.wikipedia.org/wiki/Ensemble_learning). For example, boosting combines many "weak" (high bias) models in an ensemble that has lower bias than the individual models, while bagging combines "strong" learners in a way that reduces their variance.

---

**is it possible to have a low test error then train error. If yes then how?**

[https://stats.stackexchange.com/questions/160902/is-it-possible-for-test-error-to-be-lower-than-training-error](https://stats.stackexchange.com/questions/160902/is-it-possible-for-test-error-to-be-lower-than-training-error)

If your test accuracy is higher than your train accuracy, we are likely still very far left on the training graph (where train error is high and test error is high). There are three main options for resolving that problem:

1. use an algorithm better suited for small datasets (hard to tell without knowing about your problem, but Naive Bayes is usually a good small data choice)
2. Change your model constants to fit more strongly to your training set (increasing the learning rate)
3. Get more data

---

**What is the threshold of LOF, According to wiki https://en.wikipedia.org/wiki/Local_outlier_factor The article states that if The LOF (A)>1 then it is an outlier.**

Due to the local approach, LOF is able to identify outliers in a data set that would not be outliers in another area of the data set. For example, a point at a "small" distance to a very dense cluster is an outlier, while a point within a sparse cluster might exhibit similar distances to its neighbors.
so,LOF(k) > 1 means Lower density than neighbors (Outlier)This is considered to be an outlier.

A value of approximately 1 indicates that the object is comparable to its neighbors and its not an outlier.

There is no particular threshold if Lof>1 you have to consider it as outlier.

---

**When the feature changes over time annd we Time based splitting for test and train set, then both will have different distributions. In that time, how can we make our model good?**

If we do TBS then model may learn some feature mappings with time. TBS is better when features may change with time. but we need to update our model regularly so that our **prediction distribution won't change much**.

---

**Should undersampling be done after the train test split or before ?**

After spliting the dataset and only on training dataset.

---

**Imbalnaced and balanced datasets are only in the case of classification problem. For regression problem there is no such concepts?**

Yes. There is no such concept.

---


