#Performance metrics interview questions

##Revision
[What is Accuracy?](https://www.appliedaicourse.com/lecture/11/applied-machine-learning-online-course/2978/accuracy/3/module-3-foundations-of-natural-language-processing-and-machine-learning)
[Explain about Confusion matrix, TPR, FPR, FNR, TNR?](https://www.appliedaicourse.com/lecture/11/applied-machine-learning-online-course/2979/confusion-matrix-tpr-fpr-fnr-tnr/3/module-3-foundations-of-natural-language-processing-and-machine-learning)
[What do you understand  about Precision & recall, F1-score? How would you use it?](https://www.appliedaicourse.com/lecture/11/applied-machine-learning-online-course/2980/precision-and-recall-f1-score/3/module-3-foundations-of-natural-language-processing-and-machine-learning)
[What is the ROC Curve and what is AUC (a.k.a. AUROC)?](https://www.appliedaicourse.com/lecture/11/applied-machine-learning-online-course/2981/receiver-operating-characteristic-curve-roc-curve-and-auc/3/module-3-foundations-of-natural-language-processing-and-machine-learning)
[What is Log-loss and how it helps to improve performance?](https://www.appliedaicourse.com/lecture/11/applied-machine-learning-online-course/2982/log-loss/3/module-3-foundations-of-natural-language-processing-and-machine-learning)
[Explain about R-Squared/ Coefficient of determination](https://www.appliedaicourse.com/lecture/11/applied-machine-learning-online-course/2983/r-squaredcoefficient-of-determination/3/module-3-foundations-of-natural-language-processing-and-machine-learning)
[Explain about Median absolute deviation (MAD) ?Importance of MAD?](https://www.appliedaicourse.com/lecture/11/applied-machine-learning-online-course/2984/median-absolute-deviation-mad/3/module-3-foundations-of-natural-language-processing-and-machine-learning)
[Define Distribution of errors?](https://www.appliedaicourse.com/lecture/11/applied-machine-learning-online-course/2985/distribution-of-errors/3/module-3-foundations-of-natural-language-processing-and-machine-learning)

##Interview Questions

How do we understand which  measure to use, say F1 score or accuracy or log loss etc? 

```
Are you predicting probabilities?
	Do you need class labels?
		Is the positive class more important?
			Use Precision-Recall AUC
		Are both classes important?
			Use ROC AUC
	Do you need probabilities?
		Use Brier Score and Brier Skill Score
Are you predicting class labels?
	Is the positive class more important?
		Are False Negatives and False Positives Equally Important?
			Use F1-Measure
		Are False Negatives More Important?
			Use F2-Measure
		Are False Positives More Important?
			Use F0.5-Measure
	Are both classes important?
		Do you have < 80%-90% Examples for the Majority Class? 
			Use Accuracy
		Do you have > 80%-90% Examples for the Majority Class? 
			Use G-Mean
```

---

Macro and Micro F1?

Macro F1 is just an average of F1's across sets of data.

Micro F1 is based on Micro Precision ($\frac{\sum{TP}}{\sum{TP}+\sum{FP}}$) and Micro Recall ($\frac{\sum{TP}}{\sum{TP}+\sum{FN}}$) 

Macro-average method can be used when you want to know how the system performs overall across the sets of data. You should not come up with any specific decision with this average.

On the other hand, micro-average can be a useful measure when your dataset varies in size.



Refer these docs [http://rushdishams.blogspot.com/2011/08/micro-and-macro-average-of-precision.html](http://rushdishams.blogspot.com/2011/08/micro-and-macro-average-of-precision.html), [https://towardsdatascience.com/multi-class-metrics-made-simple-part-ii-the-f1-score-ebe8b2c2ca1](https://towardsdatascience.com/multi-class-metrics-made-simple-part-ii-the-f1-score-ebe8b2c2ca1), [https://androidkt.com/micro-macro-averages-for-imbalance-multiclass-classification/](https://androidkt.com/micro-macro-averages-for-imbalance-multiclass-classification/)

---

Adjusted $R^2$ metric

$Adj\ R^2$ = $1-\frac{(1-R^2)(n-1)}{n-k-1}$ where $n$ is the count of data points and $k$ is the independent variables.

It is better to use Adjusted R-squared when there are multiple variables in the regression model. This would allow us to compare models with differing numbers of independent variables.

So, if R-squared does not increase significantly on the addition of a new independent variable, then the value of Adjusted R-squared will actually decrease.

On the other hand, if on adding the new independent variable we see a significant increase in R-squared value, then the Adjusted R-squared value will also increase.

If our R-squared value remains the same. Thus, giving us a false indication that this variable might be helpful in predicting the output. However, the Adjusted R-squared value decreased which indicated that this new variable is actually not capturing the trend in the target variable.

It is better to use Adjusted R-squared when there are multiple variables in the regression model. This would allow us to compare models with differing numbers of independent variables.

Ref : [https://www.analyticsvidhya.com/blog/2020/07/difference-between-r-squared-and-adjusted-r-squared/](https://www.analyticsvidhya.com/blog/2020/07/difference-between-r-squared-and-adjusted-r-squared/)

---

real meaning of sensitivity and specificity?

Sensitivity measure is used to determine the proportion of actual positive cases, which got predicted correctly, Specificity measure is used to determine the proportion of actual negative cases, which got predicted correctly

---


Which is more important to youâ€“ model accuracy, or model performance?
>model performance

Can you cite some examples where a false positive is important than a false negative?

>Decision based on revenue prediction of a company, where if we predicted revenue is going to increase (FP) in near future however in actual it is not the decision company might have taken on prediction basis can impact company reputation
>Places where we accept or think it is positive when infact it is not the case.


Can you cite some examples where a false negative important than a false positive?
>Generally medical test of any serious disease where ensuring no patient is wrongly classified if they are suffering with serious disease.
>>Places where we reject or think it is negative when infact it is not the case.


Can you cite some examples where both false positive and false negatives are equally important?
>when we're classifying flowers into, say, two classes - Versicolor and Virginica, so here whether Versicolor is wrongly classified as Virginica or vice-versa both are equally bad, hence equal importance is given to FP and FN.

What is the most frequent metric to assess model accuracy for classification problems?
>ROC in case of binary classification else confusion matrix. If we have probabilities, we can make use of log loss

Why is Area Under ROC Curve (AUROC) better than raw accuracy as an out-of- sample evaluation metric?
