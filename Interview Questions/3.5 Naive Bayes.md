#3.5 Naive Bayes

##Revise

- [Bayes Theorem problem:](https://youtu.be/LadMzl8MaXM  )
- [More Bayes Theorem problems:](https://www.math.upenn.edu/~mmerling/math107%20docs/practice%20on%20Bayes%20solutions.pdf http://gtribello.github.io/mathNET/bayes-theorem-problems.html http://wwwf.imperial.ac.uk/~ayoung/m2s1/WorkedExamples1.pdf)
- [What is Conditional probability?](https://www.appliedaicourse.com/lecture/11/applied-machine-learning-online-course/2989/conditional-probability/3/module-3-foundations-of-natural-language-processing-and-machine-learnin)
- [Define Independent vs Mutually exclusive events?](https://www.appliedaicourse.com/lecture/11/applied-machine-learning-online-course/2990/independent-vs-mutually-exclusive-events/3/module-3-foundations-of-natural-language-processing-and-machine-learning)
- [Explain Bayes Theorem with example?](https://www.appliedaicourse.com/lecture/11/applied-machine-learning-online-course/2991/bayes-theorem-with-examples/3/module-3-foundations-of-natural-language-processing-and-machine-learning)
- [How to apply Naive Bayes on Text data?](https://www.appliedaicourse.com/lecture/11/applied-machine-learning-online-course/2995/naive-bayes-on-text-data/3/module-3-foundations-of-natural-language-processing-and-machine-learning)
- [What is Laplace/Additive Smoothing?](https://www.appliedaicourse.com/lecture/11/applied-machine-learning-online-course/2996/laplaceadditive-smoothing/3/module-3-foundations-of-natural-language-processing-and-machine-learning)
- [Explain Log-probabilities for numerical stability?](https://www.appliedaicourse.com/lecture/11/applied-machine-learning-online-course/2997/log-probabilities-for-numerical-stability/3/module-3-foundations-of-natural-language-processing-and-machine-learning)
- [In Naive bayes how to handle  Bias and Variance tradeoff?](https://www.appliedaicourse.com/lecture/11/applied-machine-learning-online-course/2998/bias-and-variance-tradeoff/3/module-3-foundations-of-natural-language-processing-and-machine-learning)
- [What Imbalanced data?](https://www.appliedaicourse.com/lecture/11/applied-machine-learning-online-course/3000/imbalanced-data/3/module-3-foundations-of-natural-language-processing-and-machine-learning)
- [What is Outliers and how to handle outliers?](https://www.appliedaicourse.com/lecture/11/applied-machine-learning-online-course/3001/outliers/3/module-3-foundations-of-natural-language-processing-and-machine-learning)
- [How to handle Missing values?](https://www.appliedaicourse.com/lecture/11/applied-machine-learning-online-course/3002/missing-values/3/module-3-foundations-of-natural-language-processing-and-machine-learning)
- [How to Handling Numerical features (Gaussian NB)](https://www.appliedaicourse.com/lecture/11/applied-machine-learning-online-course/3003/handling-numerical-features-gaussian-nb/3/module-3-foundations-of-natural-language-processing-and-machine-learning)
- [Define Multiclass classification.?](https://www.appliedaicourse.com/lecture/11/applied-machine-learning-online-course/3004/multiclass-classification/3/module-3-foundations-of-natural-language-processing-and-machine-learning)

##Interview Question

1. Standardization / Normalization required before applying Naive Bayes?
2. When should we use KNN as compared to Naive Bayes or viceversa?

we don't need standardization or normalization for NB because NB is not distance based algorithm. Unlike KNN which can be affected by scale of features, NB just calculates the likelihoods which are independent of other features.

KNN can be used for lower dimensional dataset. NB and its variations are mainly used with text based classifications as dimensionlity is high. However, there isn't a thumb rules for any algo in ML. It is just hit and trial.

---

How will you regularise your naive bayes model?

1. Using smoothing techniques like Laplace smoothing
2. While creating the features, we can remove some features which rarely occur using some threshold so we may reduce some variance in the model.

---

Resource which clearly explain bernoulli ,gasussian , Multinomial naive bayes

[https://www.quora.com/What-is-the-difference-between-the-the-Gaussian-Bernoulli-Multinomial-and-the-regular-Naive-Bayes-algorithms](https://www.quora.com/What-is-the-difference-between-the-the-Gaussian-Bernoulli-Multinomial-and-the-regular-Naive-Bayes-algorithms)

[https://scikit-learn.org/stable/modules/naive_bayes.html#multinomial-naive-bayes](https://scikit-learn.org/stable/modules/naive_bayes.html#multinomial-naive-bayes)

---

1. Why N.B work well with categorical features compared to numerical features?
2. also what if we have mix types of features i.e categorical and numerical?which class will perform better Multinomial or Gaussian N.B ?


1. It is due to the assumption that features follow Gaussian distribution (strong) in case of numerical features
2. If we have mixed type of features,we can use Gaussian Naive Bayes,it will work better.

---

